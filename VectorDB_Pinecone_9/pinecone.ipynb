{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 0}, page_content='FUSECAP: Leveraging Large Language Models\\nfor Enriched Fused Image Captions\\nNoam Rotstein* David Bensa ¨ıd* Shaked Brody Roy Ganz Ron Kimmel\\nTechnion - Israel Institute of Technology\\n*Indicates equal contribution.\\nAbstract\\nThe advent of vision-language pre-training techniques en-\\nhanced substantial progress in the development of models for\\nimage captioning. However, these models frequently produce\\ngeneric captions and may omit semantically important im-\\nage details. This limitation can be traced back to the image-\\ntext datasets; while their captions typically offer a general\\ndescription of image content, they frequently omit salient\\ndetails. Considering the magnitude of these datasets, man-\\nual reannotation is impractical, emphasizing the need for an\\nautomated approach. To address this challenge, we leverage\\nexisting captions and explore augmenting them with visual\\ndetails using “frozen” vision experts including an object\\ndetector, an attribute recognizer, and an Optical Character\\nRecognizer (OCR). Our proposed method, FUSECAP, fuses\\nthe outputs of such vision experts with the original captions\\nusing a large language model (LLM), yielding comprehen-\\nsive image descriptions. We automatically curate a training\\nset of 12M image-enriched caption pairs. These pairs un-\\ndergo extensive evaluation through both quantitative and\\nqualitative analyses. Subsequently, this data is utilized to\\ntrain a captioning generation BLIP-based model. This model\\noutperforms current state-of-the-art approaches, producing\\nmore precise and detailed descriptions, demonstrating the\\neffectiveness of the proposed data-centric approach. We\\nrelease this large-scale dataset of enriched image-caption\\npairs for the community.\\n1. Introduction\\nThe generation of image captions that effectively cap-\\nture essential descriptive elements has been a longstanding\\ngoal in computer vision [7, 32, 34, 36, 49, 55]. In recent\\nyears, image captioning tasks [3, 44] have gained signifi-\\ncant research attention and interest due to the success of\\nVision Language (VL) models. This achievement mainly\\nstems from the ability to efficiently harness the massive\\namount of image-caption pairs accessible online, using Vi-\\nOriginal: Two men with eye glasseslooking at somethingOurs: Two bespectacled men, one with black glasses and a black and brown beard, the other with silver glasses and short brown hair, sit together with an open blue laptop on a table in front of them. A graycat lounges nearbyOriginal: Mhmm, some clouds inthe sky Ours: A woman wearing dark sunglasses stands next to a red car with a black license plate reading 166882, PRI. The car has off and round headlights, a chrome and silver bumper, a black tire, and a red door. The cloudy and white sky is visible in the background.\\nOriginal: save yourself the expense of a professional arrangement . Ours: Floral Arrangement: A colorful assortment of sunflowers, yellow, white, orange, and purple flowers, and green leaves arranged on a black and wood table.\\nOurs: A woman with blond, long hair wearing a black belt and pants attends the premiere of The Little Stranger in 2018.Original: <PERSON> 2018 : <PERSON>: The Little Stranger Premiere -01Figure 1. FUSECAPcaptions. An illustration comparing our\\nFUSECAPenriched captions with the original ground-truth captions\\nbefore the fusing process. The examples are from COCO, SBU,\\nCC, and CC12 datasets, displayed from top to bottom.\\nsion Language Pre-training (VLP) [15, 40, 53], followed\\nby task-specific fine-tuning. However, despite remarkable\\nadvancements in image captioning, current state-of-the-art\\nmodels [14, 28, 37, 38, 68, 70, 73, 81] produce captions that\\noften overlook key semantic elements. As images are rich\\n1arXiv:2305.17718v2  [cs.CV]  15 Nov 2023'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 1}, page_content='sources of information containing intricate and complex con-\\ntent, providing precise descriptions requires highly detailed\\ntextual captions.\\nWe hypothesize that the current unsatisfactory captioning\\nresults are attributed to the image-caption datasets used for\\ntraining. Captions in these datasets frequently fail to capture\\nessential elements within images and often omit fine details.\\nFor example, consider the original caption of the top image\\nin Figure 1. The caption is missing details such as the laptop\\nand cat. Since these datasets contain a massive number of\\nimage-caption pairs, manual re-annotation is unfeasible.\\nIn this work, our primary objective is to develop a frame-\\nwork that produces richer, more accurate captions for im-\\nages. To achieve this, we introduce FUSECAP, a novel\\napproach designed to automatically augment captions in ex-\\nisting datasets, thereby enhancing training data of the model.\\nThis contrasts with methods that primarily focus on improv-\\ning the caption generator model architecture and resonates\\nwith the recently surveyed data-centric artificial intelligence\\n(AI) paradigm [79, 80], which underscores the significance\\nof improving both the quality and quantity of data, rather\\nthan merely concentrating on the advancement of model de-\\nsign. We leverage the capabilities of vision experts such as\\nobject detectors [4], attribute recognizers [82], and Optical\\nCharacter Recognition (OCR) models [1, 2, 8]. The visual\\ninformation extracted by these models is intended to provide\\ncomplementary details to the original simplistic captions. By\\nharnessing the reasoning capabilities of a dedicated LLM,\\nthe outputs from the vision experts are fused with the original\\nimage caption. This results in a coherent, meaningful nat-\\nural description of images that is more comprehensive and\\ndetailed than the original caption, as illustrated in Figure 1.\\nSpecifically, we leverage ChatGPT [11] to generate “fusing”\\nexamples which are then used to fine-tune a pre-trained Flan-\\nT5 model [54]. We apply this method to enrich captions\\nof a human-annotated dataset (COCO [44]) and large-scale\\ndatasets collected from the web (CC12 [12], CC [58], and\\nSBU [65]). This process produces an enriched collection\\nconsisting of 12M image-text pairs. To confirm the quality\\nof our generated dataset, we first show that humans favor\\nthe fused captions, perceiving them as more descriptive and\\naccurate than the original captions. Furthermore, we demon-\\nstrate that the fused captions score higher on CLIPScore [26]\\n– a reference-free metric that evaluates text-image alignment\\nwithout using reference captions — compared to the origi-\\nnal ones. To further emphasize their effectiveness, we also\\nassess the captions through image-to-text and text-to-image\\nretrieval tasks.\\nTo illustrate the benefits of the proposed data-centric ap-\\nproach we capitalize on these fused captions to train a cap-\\ntion generator. We use the augmented dataset both for pre-\\ntraining and for the fine-tuning of an image captioning BLIP\\nmodel [38]. Despite having fewer parameters and using lesstraining data, our model surpasses existing state-of-the-art\\nmethods [37, 38, 46, 69, 70] in generating comprehensive\\ncaptions. This superiority is evident both in its higher CLIP-\\nScore and its improved performance in the retrieval tasks.\\nThe performance advantage is further illustrated by numer-\\nous examples.\\nOur contributions are as follows:\\n•Introducing FUSECAP- a novel approach to automati-\\ncally enrich existing image-captions datasets by fusing\\noutputs from visual experts using an LLM.\\n•Providing a large dataset of 12M caption-enriched text-\\nimage pairs for future research.\\n•Showcasing that an enriched dataset leads to models\\ncapable of generating detailed captions that effectively\\nincorporate previously overlooked key semantic ele-\\nments.\\n2. Related Efforts\\nImage-Caption Generation. Image captioning has been a\\nwidely researched topic at the intersection of computer vision\\nand natural language processing. Early strategies for caption\\ngeneration made use of retrieval-based methods [23, 27] and\\ntemplate-based methods [35], which were limited in their\\nexpressiveness [6]. The advent of deep learning marked a\\nshift in this field, as multimodal neural networks enabled\\nthe generation of higher quality captions [31]. Subsequently,\\nthe encoder-decoder framework, which essentially translates\\nan image into a sentence, became one of the favored ap-\\nproaches [20,66]. Later advancements incorporated attention\\nmechanisms for focusing on key image aspects [76,78]. Cur-\\nrent best captioning techniques are transformer-based archi-\\ntectures [63], which combine vision [21] and language [63]\\ntransformers. The recent advancements in the effectiveness\\nof image captioning can largely be credited to the introduc-\\ntion of visual language pre-training (VLP).\\nVLP uses large-scale image-text pairs to pre-train vision-\\ntext models, which are later fine-tuned for downstream tasks,\\nwith image captioning as one of the central tasks [25, 37,38].\\nVLP can be categorized [13] into single-stream [16, 41] and\\ndual-stream architectures [22, 24, 29, 53], aiming to merge\\nvisual and textual modalities into a shared embedding space.\\nEfforts exploring integrating text generative tasks for pre-\\ntraining [17, 72] have fueled the development of models like\\nBLIP [38] and recent BLIP-2 [37]. OFA [70] proposed uni-\\nfying multiple unimodal and multimodal pre-training tasks,\\nwhich led to a significant performance improvement.\\nDense captioning can be seen as a task related to ours in\\ngenerating comprehensive text for images [30,77]. However,\\nit produces multiple captions for various regions within an\\nimage, as opposed to a single descriptive caption for the\\nwhole image. Previous approaches to generating more com-\\nprehensive captions have consistently identified a common\\n2'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 2}, page_content='Vision ExpertsOriginalImage-CaptionPair\\na woman is holding a tennis racket at a game  A woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.LLM FuserAttributes ExtractorDetector\\nOCR\\nNIKEEmiratesAirlineFusedCaption(a) Fusing Enriched Captions\\n  A woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.  E woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.Caption Generation Pretrain+Finetune\\n  A woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.  A man with a gray beard and dark sunglasses stands next to a green sign with various golf course information. He holds a yellow frisbee in his hand. A tall tree stands in the background.\\nEnrichedImage-CaptionPair\\n(b) Training a Captioning Model\\nFigure 2. Our Approach Illustration. Figure 2a illustrates the automated process of enriching existing image captions using the proposed\\nFUSECAPapproach. Visual experts extract meaningful information from images, which is then fused with the original captions by an LLM\\nFuser, producing rich captions. Following this, Figure 2b illustrates the utilization of image datasets, paired with these augmented captions,\\nin both the pre-training and fine-tuning phases of a comprehensive image-captioning model.\\nchallenge – the unsatisfactory capability of existing image\\ncaptioners of providing detailed and accurate textual descrip-\\ntions of images [45, 47, 48]. These approaches focus on\\nimproving captions discriminability, whereas our method\\nseeks to enrich captions with additional, meaningful infor-\\nmation extracted from the images.\\nImage-Caption Datasets Current datasets for image-\\ncaption pairs fall into two primary categories: specifically\\nhuman-annotated datasets like COCO [44], and web-crawled\\ndatasets such as CC, CC12, and SBU Captions [12, 50, 58].\\nThe conjunction of the datasets serves as the foundation for\\nthe VLP, subsequently followed by downstream tasks fine-\\ntuning. In general, the first category of datasets is smaller in\\nsize yet exhibits substantially lower noise levels compared\\nto the second category. Both categories, however, are char-\\nacterized by relatively short and concise captions. Specific\\nexamples of these characteristics can be seen in Figure 1.\\nData-centric AI underscores the importance of refining\\ndata quality to boost model performance rather than fine-\\ntuning model designs. It prioritizes curating, labeling, and\\ncleaning data for superior training datasets [61] as well as\\nautomating its processing [79, 80]. This new emphasis sug-\\ngests that with high-quality data, even basic algorithms candeliver impressive results. [59] identified the need to enrich\\nimage-text datasets. To this end, they harnessed natural lan-\\nguage inference (NLI) to fuse multiple existing ground-truth\\ncaptions into a single one. However, this approach can oper-\\nate only on datasets with multiple ground-truth captions and\\ncannot be applied to large-scale ones with a single caption\\n(e.g., CC, CC12, and SBU). In contrast, our method can be\\napplied to any image-caption dataset.\\nLarge Language Models Large language models (LLMs)\\nhave been shown to be effective in a wide range of tasks,\\nincluding natural language inference, question answering,\\nand code generation [18,19,74]. Further, LLMs such as GPT-\\n3 [11] exhibit impressive zero and few-shot performance on\\na variety of tasks, including translation, text summarization,\\nand common sense reasoning, without further fine-tuning.\\nFew-shot abilities allow researchers to use LLMs as a tool\\nfor data generation. [10] used GPT-3 to generate instructions\\nand edited captions dataset, which is then used to train model\\nfor image editing. [57] presented an approach for training\\nLLMs to use external API calls. For example, when the\\nLLM is been asked to solve a mathematical problem, it\\ncan use an API of a calculator, instead of generating the\\noutput by itself. [57] used GPT-3 few-shot ability to curate\\n3'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 3}, page_content='a dataset of external API calls which is then been used to\\nfine-tune another LLM. [52] used GPT-4 outputs to create\\nan instruction-following dataset that can be used later to fine-\\ntune other LLMs in a supervised learning fashion. To fuse\\ntogether the original caption and the visual expert outputs, in\\nthis work, we harness the impressive zero-shot capabilities of\\nOpenAI’s ChatGPT [11]. We employ it to generate a small\\n“fusing” dataset. To establish an open-source framework that\\nscales cost-effectively, we fine-tune Flan-T5 [19], a widely\\nrecognized open-source LLM, using this data.\\nVision Experts in VLP Several works attempted to im-\\nprove VLP by incorporating object detectors or other ex-\\nperts [46] as part of their initialization [17, 41], architec-\\nture [46], pre-training data [67] or pre-training objectives\\n[42, 60, 62, 82]. In image captioning, such models demon-\\nstrate limited capabilities in generating rich captions and\\nhave not fully capitalized on the information provided by\\nvision experts. We hypothesize that this limitation stems\\ndirectly from the succinct captions in existing image-text\\ndatasets. In our data-centric approach, we focus on im-\\nproving such datasets and overcoming this limitation. An\\nexample that highlights our approach is the contrast with\\nPrismer [46]. Unlike our method, Prismer augments its\\nmodel architecture with object detectors and OCR for cap-\\ntion generation but relies on traditional captioning datasets.\\nA recent concurrent paper [71] proposed an LLM-based\\nmodel that merges captions of image segments into text. In\\ncontrast, we adopt the LLM from a data-centric perspective\\nto enhance existing caption datasets, an approach that is fol-\\nlowed by extensive evaluations for quality and consistency.\\n3. Fusing Enriched Captions\\nIn this section, we introduce FUSECAP, which is illus-\\ntrated in Figure 2a. This novel strategy is designed to au-\\ntomatically augment existing image captions by integrating\\nimportant details and inter-object relations within the image.\\nThese details are often disregarded in traditional image cap-\\ntioning datasets. First, we elaborate on our use of pre-trained\\nvision models, referred to as vision experts , for extracting\\nrelevant visual information from images. We then detail how\\nthe information gathered from these expert models is subse-\\nquently fused with the original caption through a fine-tuned\\nLLM, resulting in the enhanced captions.\\n3.1. Vision Expert Models\\nTo enrich the information found in the original caption,\\nwe employ the following vision experts:\\nObject Detection A key visual expert we rely on is an\\nobject detector model. Following the approach proposed\\nin [82], we utilize a Faster-RCNN [56] with a ResNeXt152\\n[75] backbone. This model is initially pre-trained on sev-\\neral detection datasets and then fine-tuned on the VisualGenome (VG) dataset [33]. The data used comprises more\\nthan 100K images with 1.6K classes, enabling strong gen-\\neralization abilities. We regard all objects along with their\\ncorresponding bounding boxes as valid detections, provided\\nthey exceed a detection confidence threshold. The presence\\nand position of objects in the scene provide essential details\\nto complement the caption.\\nAttributes Prediction Beyond class identification, we de-\\nrive a variety of attributes for each object in the image using\\nfeatures generated by our Faster-RCNN within each bound-\\ning box. A classification model proposed in [82] was trained\\non annotations from the VG dataset, which covers a broad\\narray of 400 distinct attributes. These attributes encapsulate\\nvarious aspects of the objects, including size, condition, and\\ncolor. For each object, we only consider attributes predicted\\nwith a confidence level above the threshold.\\nText Detection and Recognition Text within images often\\ncontains critical contextual information, leading to a line\\nof different text-image tasks [9, 25]. To incorporate textual\\ninformation, we utilize robust pre-trained OCR models to\\ndetect and extract characters. We first identify text within\\na scene with CRAFT [5], a robust scene-text detector. We\\nthen apply Parseq [8], a state-of-the-art scene text recognizer,\\nto decode the text within the bounding boxes generated by\\nthe text detector. To avoid contamination, we do not apply\\nOCR methods on datasets presenting watermarks, such as\\nthe CC [58] and CC12 [12].\\n3.2. LLM Fuser\\nTo generate natural and coherent captions, which are es-\\nsential for the caption generation task, we utilize a large lan-\\nguage model (LLM). We leverage a specifically fine-tuned\\nLLM to fuse insights from various vision experts into the\\noriginal caption, creating a single coherent and fluent de-\\nscription. The LLM’s advanced reasoning capabilities allow\\nus to articulate the semantic relationships among objects and\\nto seamlessly integrate the diverse knowledge provided by\\nthese experts. Consequently, the output captures the essence\\nof the visual content while the impressive generative capacity\\nof LLMs ensures these captions remain coherent and natural.\\nTo train the LLM, we create a small “fusing” dataset with\\nChatGPT [11], and then use it to fine-tune the open-source\\nFlanT5-XL model [19].\\nChatGPT Annotation We leverage the zero-shot capa-\\nbilities of ChatGPT to generate a “fusing” dataset of 20K\\nexamples, which is then used to fine-tune an open-source\\nLLM. By employing this approach, we establish a framework\\nbased on open-source models that can generate a large-scale\\ndataset at a reasonable cost. To produce enriched captions\\nwith ChatGPT, we first extract information from the visual\\n4'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 4}, page_content='OFA:a surfer in a wetsuit rides a wave.GIT: woman in a wetsuit is surfing on a waveBLIP2: a woman in a wet suit riding a wave on a surfboardOurs: a barefoot surfer with long brown hair rides a white wave on a white surfboard, extending their arm and hand for balance\\nPrismer: A woman riding a wave on top of a surfboard\\nOFA: motorcycle parked on the beachGIT: a red motorcycle parked on a road near a beachBLIP2: a red motorcycle parked in a parking lot next to a fenceOurs: a red motorcycle with a leather and black seat is parked on the side of the road, surrounded by a wood fence and tall palm trees the clear blue sky provides a serene backdropPrismer: A red motorcycle parked on the side of a road.BLIP2: a boat is in the water and birds are on the shore Ours: a serene sunset scene with a small boat anchored in calm waters, surrounded by a flock of birds and a dark cloud in the sky\\nOFA: a flock of birds are swimming in the water at sunsetGIT: a large body of water with a boat in the distance.Prismer:  A large body of water filled with birds under a cloudy sky.\\nOurs: a snowboarder glides down a snow -covered mountain under a gray sky, wearing tan and khaki pants and casting a dark shadow\\nBLIP2: a group of people on snowboards on a snowy slopOFA: a man riding a snowboard down a snow coveredslopeGIT: a group of people riding snowboards on top of a snow coveredslope.Prismer: A group of people on snowboards and skis in the snow.Figure 3. Image Captioning Results. While top-performing captioning models tend to provide concise and oversimplified captions, our\\nmodel outputs rich captions that better describe the images.\\nexperts presented in Section 3.1. The objects and their cor-\\nresponding attributes and detected texts are then ordered\\nfrom left to right based on their bounding boxes, providing\\nbasic spatial context. The exact prompt can be found in the\\nappendix. We generated 20K examples of such enriched\\ncaptions from CC, CC12, SBU, and COCO datasets, which\\nserved as training data for the FlanT5-XL model.\\nLLM Fine-tuning Using the “fusing” dataset we created,\\nwe fine-tuned Flan-T5 [19], a variant of the T5 encoder-\\ndecoder model [54], that has been extensively fine-tuned\\non numerous tasks to achieve exceptional performance on\\ninstruction-based tasks. Specifically, we utilized the Flan-T5-\\nXL checkpoint and fine-tuned it to our curated fuse dataset.\\nDuring the fine-tuning process, the original caption was con-\\ncatenated with the output of the visual experts and served as\\nthe model input, while the enriched caption was designated\\nas the target. The hyperparameters we used for this section\\ncan be found in the appendix. After this phase, the fine-tuned\\nmodel can be used to generate an enriched captions dataset.\\n4. Training a Captioning Model\\nWe apply FUSECAP, presented in Section 3, to the COCO,\\nSBU, CC, and CC12 datasets, yielding 12 million aug-\\nmented image-caption pairs. Following our data-centric\\napproach, and to demonstrate the effectiveness of this aug-\\nmented dataset, we use it to train a captioning model based\\non the BLIP architecture [38] (Figure 2b). Adopting the\\ntraining strategy from the original BLIP paper, we first per-\\nform vision-language pre-training for 20epochs , optimizing\\nthree objectives: (a) Image-Text Contrastive Loss (ITC)that aligns visual and textual features by contrasting between\\nmatching and non-matching image-text pairs, (b) Image-\\nText Matching Loss (ITM) that classifies image-text pairs\\nas matched or unmatched, and (c) Generative Language\\nModeling Loss (LM) that generates textual captions from\\nimages, which are compared to their ground truth captions.\\nFollowing the pre-training phase, adhering to common prac-\\ntice in VLP [37,38,81], we fine-tune our model for a supple-\\nmentary 5epochs on the enriched COCO dataset, utilizing\\nsolely LM loss. To allow the generation of more comprehen-\\nsive captions, we increase the context length over that used\\nin the original BLIP model from 30to60tokens. The hyper-\\nparameters for this section are detailed in the appendix.\\n5. Experiments\\nWe evaluate two sets of image captions. The first set\\nconsists of the fused captions from the FUSECAPdataset,\\ndescribed in Section 3. The second set includes captions\\ngenerated by the trained captioning model discussed in Sec-\\ntion 4. The objective for both sets is to produce captions\\nthat are descriptive and accurate. Traditionally, image cap-\\ntioning methods are assessed with n-gram-based metrics like\\nBLEU [51], CIDEr [64], and ROUGE [43]. These metrics\\ncompare the tested captions to reference captions, assum-\\ning the latter represents the ideal image descriptions that\\nthe tested captions aim for. However, as highlighted earlier,\\ncaptions in existing datasets often fail to provide a compre-\\nhensive description of images. This implies that given our\\nemphasis on enhancing descriptiveness, relying on broad\\nand non-specific reference captions for evaluation would be\\ninappropriate. Consequently, n-gram-based metrics do not\\n5'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 5}, page_content='effectively measure or promote this quality [84].\\nWe, therefore adopt CLIPScore [26], a reference-free met-\\nric that measures the alignment between textual and visual\\nembeddings generated by a pre-trained CLIP model [53].\\nSince CLIPScore is not dependent on a reference caption, its\\nscore is not restricted by the descriptiveness of the original\\ncaptions. Moreover, in terms of accuracy, it has been shown\\nto exhibit a higher correlation with human judgments than\\nn-gram-based metrics [26].\\nBesides evaluating enriched captions with CLIPScore, we\\nconsider the ability to perform image-text retrieval as a perti-\\nnent metric for caption evaluation. Comprehensive captions\\nshould inherently serve as distinct image descriptors, thereby\\nimproving retrieval precision. To further assess the quality\\nof captions in the FUSECAPdataset, we conducted a human\\nevaluation study.\\n5.1. FuseCap Dataset\\nIn this subsection, we evaluate the enriched captions pro-\\nduced by the caption augmentation approach introduced\\nin the Section 3. In particular, we carry out a qualitative\\nhuman-study alongside quantitative evaluations. Examples\\nof enriched captions via FuseCap are showcased in Figure 1.\\nQualitative Evaluation We conducted a thorough human\\nevaluation study to assess the ability of our enriched captions\\nto be both descriptive and relevant to the images. Specifically,\\nwe randomly sampled 400 pairs from the COCO dataset and\\nprovided 40 participants with (1) the original caption and\\n(2) the enriched caption. Our study engaged a pool of ran-\\ndom internet users as participants. To minimize biases and\\nensure an impartial evaluation, they completed the survey\\nunaware of the specific research objectives or goals. The\\nevaluators were asked the following: “Does caption 2 pro-\\nvide an additional meaningful and truthful description of\\nthe image compared to caption 1?” . As the enriched cap-\\ntions are much more detailed, as can be seen in Figure 1, the\\nDataset Captions Mean V oting\\nCOCOOriginal 76.7 31.7%\\nFUSECAP 80.3 67.6%\\nSBUOriginal 71.9 32.1%\\nFUSECAP 75.5 60.2%\\nCCOriginal 72.6 34.7%\\nFUSECAP 75.4 59.7%\\nTable 1. FUSECAPdata quantitative evaluation. CLIPScore-\\nbased comparison between the original captions of common image-\\ntext datasets with our enriched ones. “Mean” indicated the mean\\nCLIPScore and “V oting” expresses CLIP’s preference in a one-\\nvs-one setting. As can be seen, FUSECAPobtains significantly\\nimproved results in both metrics.question focuses on whether they are accurate. Our study\\nresults indicate that participants find the enriched captions\\nat least as good as the original ones in 72.9%of the images.\\nThis finding highlights the proposed method’s effectiveness\\nin enhancing the captions’ descriptiveness while preserving\\nalignment and relevancy to the images.\\nQuantitative Evaluation We evaluate the enriched cap-\\ntions in comparison to the original captions for each dataset\\nunder consideration. To this end, we randomly selected 5000\\nimages from each dataset and report the CLIPScore obtained\\nwith both types of captions. As depicted in Table 1, the\\nenriched captions generated by our proposed method con-\\nsistently achieve a higher CLIPScore (Mean) on average by\\n4.6%. In addition, given an image and two captions (original\\nand enriched), we utilize CLIP to measure which caption\\nis preferred. We evaluate this on the different datasets and\\nsummarize it under “V oting”, which our captions outperform\\ncurrent ones on average by 29.7%. These results demon-\\nstrate the effectiveness of our approach in generating en-\\nriched captions that better reflect the content of the images.\\nImage-Text Retrieval To further demonstrate the descrip-\\ntiveness and accuracy of the FUSECAPdata, we assess its\\nperformance in the image-text retrieval task. This task in-\\nvolves matching images to text queries and vice versa. If\\nthe enriched data is descriptive and accurate, retrieval per-\\nformance should improve since the additional details can\\nserve as a discriminative factor in establishing these cor-\\nrespondences. Our training methodology aligns with the\\noriginal BLIP model. After pre-training the model on a\\nlarge-scale dataset as in Section 4, the model is fine-tuned\\nfor image-text retrieval on the COCO training set using both\\nITC and ITM losses. For the inference step, we employ the\\nmethod proposed by [39], previously integrated into BLIP.\\nThis method involves the selection of Kcandidates based\\non feature similarity, followed by their re-ranking using re-\\nspective ITM values. We report R@Nthat corresponds to\\nthe accuracy of retrieving the true text/image among the top\\nNretrieved results. The details of the competing models\\nare provided in Section 5.2, with the exception that the ver-\\nsions discussed here have been fine-tuned for the retrieval\\ntasks. As illustrated in Table 2, the use of fused captions\\ncontributes significantly to the enhancement of retrieval per-\\nformance on the COCO test set1. For example, compared to\\nthe model trained on corresponding non-enriched data, the\\nR@1score for image-to-text retrieval increased by 22.1%,\\nand for text-to-image retrieval, it increased by 34.8%.\\n5.2. Caption Generation\\nWe fine-tune the BLIP model that was pre-trained on\\nthe complete FUSECAPdataset for captioning, using the\\n1While we used enriched captions for training and testing BLIP FUSECAP,\\nfor the other baselines we used the original dataset without enrichment.\\n6'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 6}, page_content='COCO Retrieval\\nimg→text text →img\\nModel R@1 R@5 R@10 R@1 R@5 R@10\\nBLIP† 75.1 92.7 96.4 58.2 82.4 89.2\\nBLIP-L 82.4 95.4 97.9 65.2 86.3 91.8\\nBLIP2 85.4 97.0 98.5 68.3 87.7 92.6\\nBLIP∗\\nFUSECAP 97.2 99.5 99.9 93.0 97.4 98.3\\nTable 2. Image-text retrieval results. Performance on COCO\\nretrieval (test sets). The “*” symbol indicates that the model was\\ntrained and tested on our enriched dataset. These results attest that\\ngiven rich captions, BLIP FUSECAPsignificantly outperforms existing\\nmethods, which utilize standard captions.\\nenriched COCO dataset. We refer to this fine-tuned version\\nas BLIP FUSECAP. To assess the effectiveness of the captioner,\\nand, by extension, the FUSECAPdata it is trained on, we\\nthoroughly compare our results to various state-of-the-art\\ncaptioning models using CLIPScore. In particular, we con-\\nsider the following baselines:\\n•BLIP†[38]: An original BLIP model, pre-trained and\\nfine-tuned on the same image set as BLIP FUSECAP, which\\nuses original captions, in contrast to BLIP FUSECAP\\nwhich uses the enriched captions. To guarantee a fair\\ncomparison, we set configuration parameters identical\\nto those implemented in BLIP FUSECAP.\\n•BLIP-L : A large version of BLIP, which was pre-\\ntrained on a dataset comprising 129M images prior\\nto the captioning fine-tuning. Among all models pre-\\nsented in the original BLIP paper, this model achieves\\nsuperior captioning results.\\n•BLIP2-G-OPT 2.7[37]: A state-of-the-art captioning\\nmodel that integrates a large frozen vision backbone\\n(ViT G) along with a frozen LLM (OPT [83]), trained\\non the datasets considered for BLIP-L.\\n•OFA [70]: A state-of-the-art image captioning method\\ntrained on unimodal and multimodal pre-training tasks\\nand fine-tuned for captioning.\\n•GIT [69]: A state-of-the-art vision-language model,\\nwith an image encoder and text decoder architecture.\\nIt is scaled up in terms of both pre-training data and\\nmodel size, and it is fine-tuned for image captioning.\\n•Prismer [46]: A Vision-Language Model with Multi-\\nModal Experts such as an object detector and an OCR\\nmodel. Prismer adopts a strategy that could be seen as\\ndual to ours, integrating vision experts into the model\\narchitecture, as opposed to leveraging them to enrich\\nthe training data.Model Images Parameters Val Test\\nBLIP† 12M 247M 75.2 75.3\\nBLIP-L 129M 470M 76.1 76.0\\nOFA 20M 470M 76.6 76.4\\nGIT 800M 700M 77.1 77.0\\nBLIP2-G-OPT 2.7 129M 3.8B 77.8 77.5\\nPrismer 13M 1.6B 76.7 76.7\\nBLIP FUSECAP 12M 247M 78.3 78.5\\nTable 3. Image captioning results. CLIPScore of leading models\\nand our approach on the COCO captions dataset. Our model outper-\\nforms much larger models that have pre-trained with significantly\\nmore training data.\\nBuilding on the methodology outlined in Section 5, we\\nevaluate the performance of our models using the mean\\nCLIPScore metric. As shown in Table 3, our model not only\\noutperforms the BLIP †model by 4.3%but also surpasses the\\nbest performing among other models by 1.3%. This improve-\\nment over models with a considerably larger parameter count\\nthat have been trained on significantly more image-caption\\npairs underscores the effectiveness of our data centric ap-\\nproach. Furthermore, as illustrated in Figure 3, our approach\\nexhibits clear superiority in the generation of captions, pro-\\nducing descriptions with greater semantic detail compared\\nto those generated by competing models. Our results sur-\\npassing Prismer indicate that for generating comprehensive\\nimage captions, leveraging vision experts to enrich data can\\nbe more beneficial than incorporating them directly into the\\nmodel architecture. Additional examples showcasing the\\nperformance and capabilities of our model can be found in\\nthe supplementary material.\\nImage-Text Retrieval To further assess the quality of\\nthe generated captions, we once again utilize the image-\\ntext retrieval task, following the method detailed in Sec-\\ntion 5.1. This time, however, we consider captions not\\nfrom the ground truth COCO or the “ground truth” enriched\\nCOCO datasets. Instead, they are produced by the BLIP †\\nand BLIP FUSECAPcaption generators. Captions from BLIP †\\nand BLIP FUSECAPare assessed using retrieval models fine-\\ntuned on original and enriched captions, respectively, both of\\nthese retrieval models are the ones discussed in Section 5.1.\\nTable 4 shows that by using the captions from the captioners\\ninstead of the ground truth captions, BLIP FUSECAPretains\\nits superiority over BLIP †in both image-to-text and text-\\nto-image retrievals. Impressively, using the BLIP FUSECAP\\ncaptions, the retrieval performance is on par with results\\nachieved using the ground truth enriched captions in Table 2.\\nIn contrast, BLIP †exhibits a significant performance drop\\nwhen compared to its results with the corresponding ground\\ntruth captions.\\n7'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 7}, page_content='COCO Retrieval\\nimg→text text →img\\nModel R@1 R@5 R@10 R@1 R@5 R@10\\nBLIP†56.3 83.0 90.3 54.5 81.2 88.7\\n-18.8% -9.7% -6.1% -3.7% -1.2% -0.5%\\nBLIP∗\\nFUSECAP95.0 98.8 99.2 94.5 98.7 99.3\\n-2.2% -0.7% -0.7% +1.5% +1.3% +1%\\nTable 4. Image-text retrieval results using generated captions.\\nPerformance on COCO retrieval (test sets) using captions generated\\nby the models presented in Section 4 rather than ground truth\\ncaptions. The models used for evaluation are the ones referenced in\\nSection 5.1. The highlighted percentages indicate the performance\\ndifference when compared to the use of corresponding ground truth\\ncaptions in Table 2. Notably, BLIP FUSECAPcaptions excel over\\nBLIP †, a trend consistent with ground truth captions. Moreover,\\nBLIP FUSECAPmaintains a performance parity with the ground truth\\ndatasets, while BLIP †shows a noticeable decline in performance.\\n5.3. Large-Scale Data Influence\\nTypical VLP frameworks involve pre-training vision-\\nlanguage models on image-caption pairs before fine-tuning\\nthem on downstream tasks. Given this structure, one might\\nargue that the generation of enriched captions could serve\\nas an additional downstream task. This would suggest using\\nthe standard captions dataset for pre-training, followed by\\nfine-tuning on a smaller enriched dataset. Yet, as mentioned\\nin Section 5.2, our approach with the BLIP FUSECAPmodel\\ndeviates from this by pre-training on enriched datasets in-\\nstead of the conventional ones. In this section, we evaluate\\nthe impact of this deviation.\\nWe compare the performance of models that are pre-\\ntrained on both the standard and enriched datasets. Both\\nmodels are subsequently fine-tuned using the enriched\\nCOCO dataset. Unlike the evaluation method detailed in\\nSection 5, which is reference-free, this section employs con-\\nventional, reference-based evaluation metrics, as both mod-\\nels are fine-tuned and tested using the same dataset. Table 5\\nreveals that the model pre-trained on the enriched captions\\noutperforms its counterpart, which was pre-trained on the\\noriginal captions, across all metrics when both are fine-tuned\\non the augmented COCO dataset. This outcome underscores\\nthe importance of using large-scale enriched datasets for\\npre-training and validates our automated dataset creation\\napproach. Interestingly, the model that is pre-trained on the\\nenriched captions and fine-tuned using the standard COCO\\ndataset performs comparably to the model pre-trained on the\\noriginal captions and fine-tuned on the same dataset.\\n6. Limitations\\nIn the human evaluation study (Section 5.1), a subset of\\nparticipants favored the original caption. This may be due to\\nour fusing process occasionally missing inter-element depen-Pre-training\\nDataFine-tune\\n+Test DataB@4 CIDEr SPICE\\nStandard Standard 37.8 126.5 22.9\\nFUSECAP Standard 38.4 128.7 23.0\\nStandard FUSECAP 35.4 111.4 25.0\\nFUSECAP FUSECAP 37.3 123.1 26.8\\nTable 5. Influence of Large-Scale Data. Pre-training with the\\nFUSECAPdataset significantly outperforms pre-training on stan-\\ndard data when fine-tuned for enriched caption generation. This\\nunderscores the value of the large-scale FUSECAPdataset and its\\nautomated creation approach. Additionally, when fine-tuned for\\nstandard captioning, the FUSECAPpre-trained model is comparable\\nto, if not slightly better than, the standard data pre-trained model.\\ndencies, as seen in Figure 1, where the cat’s position between\\nthe man and the laptop was not captured. Future research\\nmight integrate finer visual details, like segmentation, and\\nrefine the LLM fuser accordingly.\\nEthical considerations FUSECAPdataset was constructed\\nusing FlanT5-XL model [19], which was trained on unfil-\\ntered data potentially laden with explicit content or inherent\\nbiases. Consequently, the proposed dataset may replicate\\nbiases from the original model.\\n7. Conclusions\\nIn this paper, we address the problem of generating highly\\ndescriptive and detailed image captions. We observe that\\nexisting state-of-the-art methods produce short and often\\noversimplified captions that fail to capture the intricate de-\\ntails in images. We hypothesize that this is due to datasets\\nlimitation, i.e.,existing training data composed of concise\\ncaptions. Thus, the tendency to provide such captions is\\ndistilled into the trained models, regardless of their architec-\\nture or training method. Therefore, we introduce a novel and\\ngeneric data-centric automated strategy to enrich existing im-\\nage captions, termed FUSECAP. Specifically, this approach\\nharnesses visual experts to extract meaningful information\\nfrom images and an LLM to fuse such data into the existing\\ncaptions, yielding enriched ones. We apply FUSECAPto dif-\\nferent widespread datasets and generate 12M image-enriched\\ncaption pairs. The augmented image captions quality is eval-\\nuated qualitatively using a human evaluation survey, and\\nquantitatively, using different evaluation methods. Finally,\\nwe demonstrate the effectiveness of the enriched data by\\nutilizing it to train an image captioning model, which outper-\\nforms significantly larger state-of-the-art methods. We posit\\nthat our research highlights the marked potential of LLMs\\nin enhancing powerful data-centric approaches in computer\\nvision.\\n8'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 8}, page_content='References\\n[1]Aviad Aberdam, David Bensa ¨ıd, Alona Golts, Roy Ganz,\\nOren Nuriel, Royee Tichauer, Shai Mazor, and Ron Litman.\\nClipter: Looking at the bigger picture in scene text recogni-\\ntion. arXiv preprint arXiv:2301.07464 , 2023. 2\\n[2]Aviad Aberdam, Roy Ganz, Shai Mazor, and Ron Litman.\\nMultimodal semi-supervised learning for text recognition.\\narXiv preprint arXiv:2205.03873 , 2022. 2\\n[3]Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste-\\nfan Lee, and Peter Anderson. Nocaps: Novel object caption-\\ning at scale. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 8948–8957, 2019. 1\\n[4]Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney,\\nMark Johnson, Stephen Gould, and Lei Zhang. Bottom-up\\nand top-down attention for image captioning and visual ques-\\ntion answering. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 6077–6086,\\n2018. 2\\n[5]Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun,\\nand Hwalsuk Lee. Character region awareness for text detec-\\ntion. In Proceedings of the IEEE/CVF conference on com-\\nputer vision and pattern recognition , pages 9365–9374, 2019.\\n4, 13\\n[6]Shuang Bai and Shan An. A survey on automatic image\\ncaption generation. Neurocomputing , 311:291–304, 2018. 2\\n[7]K. Barnard and D. Forsyth. Learning the semantics of words\\nand pictures. In Proceedings Eighth IEEE International Con-\\nference on Computer Vision. ICCV 2001 , volume 2, pages\\n408–415 vol.2, 2001. 1\\n[8]Darwin Bautista and Rowel Atienza. Scene text recognition\\nwith permuted autoregressive sequence models. In European\\nConference on Computer Vision , pages 178–196, Cham, 10\\n2022. Springer Nature Switzerland. 2, 4, 13\\n[9]Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\\nMarc ¸al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\\nthenis Karatzas. Scene text visual question answering. In\\nProceedings of the IEEE/CVF international conference on\\ncomputer vision , pages 4291–4301, 2019. 4\\n[10] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\\nstructpix2pix: Learning to follow image editing instructions.\\narXiv preprint arXiv:2211.09800 , 2022. 3\\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\\nmodels are few-shot learners. Advances in neural information\\nprocessing systems , 33:1877–1901, 2020. 2, 3, 4\\n[12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\\ntraining to recognize long-tail visual concepts. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 3558–3568, 2021. 2, 3, 4\\n[13] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi\\nChen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: A survey on\\nvision-language pre-training. Machine Intelligence Research ,\\n20(1):38–56, 2023. 2[14] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni,\\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\\nGaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-\\nbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\\nBurcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,\\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu\\nSoricut. Pali: A jointly-scaled multilingual language-image\\nmodel. CoRR , 2022. 1\\n[15] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\\nUNITER: universal image-text representation learning. In An-\\ndrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael\\nFrahm, editors, Computer Vision - ECCV 2020 - 16th Euro-\\npean Conference, Glasgow, UK, August 23-28, 2020, Pro-\\nceedings, Part XXX . Springer, 2020. 1\\n[16] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\\nUniversal image-text representation learning. In Computer\\nVision–ECCV 2020: 16th European Conference, Glasgow,\\nUK, August 23–28, 2020, Proceedings, Part XXX , pages 104–\\n120. Springer, 2020. 2\\n[17] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying\\nvision-and-language tasks via text generation. In Interna-\\ntional Conference on Machine Learning , pages 1931–1942.\\nPMLR, 2021. 2, 4\\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann,\\net al. Palm: Scaling language modeling with pathways. arXiv\\npreprint arXiv:2204.02311 , 2022. 3\\n[19] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,\\nYi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa\\nDehghani, Siddhartha Brahma, et al. Scaling instruction-\\nfinetuned language models. arXiv preprint arXiv:2210.11416 ,\\n2022. 3, 4, 5, 8, 13\\n[20] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\\nMarcus Rohrbach, Subhashini Venugopalan, Kate Saenko,\\nand Trevor Darrell. Long-term recurrent convolutional net-\\nworks for visual recognition and description. In Proceedings\\nof the IEEE conference on computer vision and pattern recog-\\nnition , pages 2625–2634, 2015. 2\\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020. 2\\n[22] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang\\nWang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu\\nYuan, Nanyun Peng, et al. An empirical study of training\\nend-to-end vision-and-language transformers. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 18166–18176, 2022. 2\\n[23] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Pe-\\nter Young, Cyrus Rashtchian, Julia Hockenmaier, and David\\nForsyth. Every picture tells a story: Generating sentences\\n9'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 9}, page_content='from images. In Computer Vision–ECCV 2010: 11th Eu-\\nropean Conference on Computer Vision, Heraklion, Crete,\\nGreece, September 5-11, 2010, Proceedings, Part IV 11 , pages\\n15–29. Springer, 2010. 2\\n[24] Roy Ganz and Michael Elad. Clipag: Towards generator-free\\ntext-to-image generation. arXiv preprint arXiv:2306.16805 ,\\n2023. 2\\n[25] Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon,\\nShai Mazor, and Ron Litman. Towards models that can see\\nand read. arXiv preprint arXiv:2301.07389 , 2023. 2, 4\\n[26] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\\nand Yejin Choi. Clipscore: A reference-free evaluation metric\\nfor image captioning, 2022. 2, 6\\n[27] Micah Hodosh, Peter Young, and Julia Hockenmaier. Fram-\\ning image description as a ranking task: Data, models and\\nevaluation metrics. Journal of Artificial Intelligence Research ,\\n47:853–899, 2013. 2\\n[28] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\\nZicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-\\nlanguage pre-training for image captioning. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 17980–17989, 2022. 1\\n[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\\nDuerig. Scaling up visual and vision-language representation\\nlearning with noisy text supervision. In International Confer-\\nence on Machine Learning , pages 4904–4916. PMLR, 2021.\\n2\\n[30] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap:\\nFully convolutional localization networks for dense caption-\\ning. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pages 4565–4574, 2016. 2\\n[31] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\\nments for generating image descriptions. In Proceedings of\\nthe IEEE conference on computer vision and pattern recogni-\\ntion, pages 3128–3137, 2015. 2\\n[32] Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep frag-\\nment embeddings for bidirectional image sentence mapping.\\nInAdvances in Neural Information Processing Systems 27:\\nAnnual Conference on Neural Information Processing Sys-\\ntems 2014, December 8-13 2014, Montreal, Quebec, Canada ,\\n2014. 1\\n[33] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\\nConnecting language and vision using crowdsourced dense\\nimage annotations. International journal of computer vision ,\\n123:32–73, 2017. 4\\n[34] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li,\\nYejin Choi, Alexander C. Berg, and Tamara L. Berg. Baby\\ntalk: Understanding and generating simple image descriptions.\\nInThe 24th IEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2011, Colorado Springs, CO, USA, 20-25\\nJune 2011 , 2011. 1\\n[35] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik\\nDhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L\\nBerg. Babytalk: Understanding and generating simple im-age descriptions. IEEE transactions on pattern analysis and\\nmachine intelligence , 35(12):2891–2903, 2013. 2\\n[36] Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,\\nTamara L. Berg, and Yejin Choi. Collective generation of\\nnatural image descriptions. In The 50th Annual Meeting of the\\nAssociation for Computational Linguistics, Proceedings of\\nthe Conference, July 8-14, 2012, Jeju Island, Korea - Volume\\n1: Long Papers , 2012. 1\\n[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\\n2: Bootstrapping language-image pre-training with frozen\\nimage encoders and large language models. arXiv preprint\\narXiv:2301.12597 , 2023. 1, 2, 5, 7\\n[38] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:\\nBootstrapping language-image pre-training for unified vision-\\nlanguage understanding and generation. In International Con-\\nference on Machine Learning , pages 12888–12900. PMLR,\\n2022. 1, 2, 5, 7, 13\\n[39] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq\\nJoty, Caiming Xiong, and Steven Chu Hong Hoi. Align\\nbefore fuse: Vision and language representation learning\\nwith momentum distillation. Advances in neural information\\nprocessing systems , 34:9694–9705, 2021. 6\\n[40] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare,\\nShafiq R. Joty, Caiming Xiong, and Steven Chu-Hong Hoi.\\nAlign before fuse: Vision and language representation learn-\\ning with momentum distillation. In Marc’Aurelio Ranzato,\\nAlina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jen-\\nnifer Wortman Vaughan, editors, Advances in Neural Infor-\\nmation Processing Systems 34: Annual Conference on Neural\\nInformation Processing Systems 2021, NeurIPS 2021, Decem-\\nber 6-14, 2021, virtual , 2021. 1\\n[41] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\\nand Kai-Wei Chang. Visualbert: A simple and perfor-\\nmant baseline for vision and language. arXiv preprint\\narXiv:1908.03557 , 2019. 2, 4\\n[42] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\\nWei, et al. Oscar: Object-semantics aligned pre-training for\\nvision-language tasks. In Computer Vision–ECCV 2020: 16th\\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\\nProceedings, Part XXX 16 , pages 121–137. Springer, 2020. 4\\n[43] Chin-Yew Lin. Rouge: A package for automatic evaluation\\nof summaries. In Text summarization branches out , pages\\n74–81, 2004. 5\\n[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part\\nV 13, pages 740–755. Springer, 2014. 1, 2, 3\\n[45] Lixin Liu, Jiajun Tang, Xiaojun Wan, and Zongming Guo.\\nGenerating diverse and descriptive image captions using vi-\\nsual paraphrases. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision , pages 4240–4249,\\n2019. 3\\n[46] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei\\nXiao, and Anima Anandkumar. Prismer: A vision-language\\n10'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 10}, page_content='model with an ensemble of experts. arXiv preprint\\narXiv:2303.02506 , 2023. 2, 4, 7\\n[47] Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, and Xi-\\naogang Wang. Show, tell and discriminate: Image captioning\\nby self-retrieval with partially labeled data. In Proceedings of\\nthe European conference on computer vision (ECCV) , pages\\n338–354, 2018. 3\\n[48] Ruotian Luo, Brian Price, Scott Cohen, and Gregory\\nShakhnarovich. Discriminability objective for training de-\\nscriptive captions. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 6964–6974,\\n2018. 3\\n[49] Rebecca Mason and Eugene Charniak. Nonparametric\\nmethod for data-driven image captioning. In Proceedings\\nof the 52nd Annual Meeting of the Association for Computa-\\ntional Linguistics, ACL 2014, June 22-27, 2014, Baltimore,\\nMD, USA, Volume 2: Short Papers , 2014. 1\\n[50] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text:\\nDescribing images using 1 million captioned photographs.\\nAdvances in neural information processing systems , 24, 2011.\\n3\\n[51] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\\nZhu. Bleu: a method for automatic evaluation of machine\\ntranslation. In Proceedings of the 40th annual meeting of the\\nAssociation for Computational Linguistics , pages 311–318,\\n2002. 5\\n[52] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley,\\nand Jianfeng Gao. Instruction tuning with gpt-4. arXiv\\npreprint arXiv:2304.03277 , 2023. 4\\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 1, 2, 6\\n[54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. Exploring the limits of transfer learning with\\na unified text-to-text transformer. The Journal of Machine\\nLearning Research , 21(1):5485–5551, 2020. 2, 5\\n[55] Krishnan Ramnath, Simon Baker, Lucy Vanderwende, Mo-\\ntaz Ahmad El-Saban, Sudipta N. Sinha, Anitha Kannan, No-\\nran Hassan, Michel Galley, Yi Yang, Deva Ramanan, Alessan-\\ndro Bergamo, and Lorenzo Torresani. Autocaption: Auto-\\nmatic caption generation for personal photos. In IEEE Winter\\nConference on Applications of Computer Vision, Steamboat\\nSprings, CO, USA, March 24-26, 2014 , 2014. 1\\n[56] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. Advances in neural information process-\\ning systems , 28, 2015. 4\\n[57] Timo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\\ncedda, and Thomas Scialom. Toolformer: Language mod-\\nels can teach themselves to use tools. arXiv preprint\\narXiv:2302.04761 , 2023. 3\\n[58] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\\nSoricut. Conceptual captions: A cleaned, hypernymed, im-age alt-text dataset for automatic image captioning. In Pro-\\nceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages\\n2556–2565, 2018. 2, 3, 4\\n[59] Zhan Shi, Hui Liu, and Xiaodan Zhu. Enhancing descrip-\\ntive image captioning with natural language inference. In\\nProceedings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume\\n2: Short Papers) , pages 269–277, 2021. 3\\n[60] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\\nlinguistic representations. arXiv preprint arXiv:1908.08530 ,\\n2019. 4\\n[61] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\\nnav Gupta. Revisiting unreasonable effectiveness of data in\\ndeep learning era. In Proceedings of the IEEE international\\nconference on computer vision , pages 843–852, 2017. 3\\n[62] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality\\nencoder representations from transformers. arXiv preprint\\narXiv:1908.07490 , 2019. 4\\n[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. Advances in neural\\ninformation processing systems , 30, 2017. 2\\n[64] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\\nParikh. Cider: Consensus-based image description evalu-\\nation. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pages 4566–4575, 2015. 5\\n[65] Tom ´as F Yago Vicente, Le Hou, Chen-Ping Yu, Minh Hoai,\\nand Dimitris Samaras. Large-scale training of shadow detec-\\ntors with noisily-annotated shadow examples. In Computer\\nVision–ECCV 2016: 14th European Conference, Amsterdam,\\nThe Netherlands, October 11-14, 2016, Proceedings, Part VI\\n14, pages 816–832. Springer, 2016. 2\\n[66] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru\\nErhan. Show and tell: A neural image caption generator. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition , pages 3156–3164, 2015. 2\\n[67] Alex Jinpeng Wang, Pan Zhou, Mike Zheng Shou, and\\nShuicheng Yan. Position-guided text prompt for vision-\\nlanguage pre-training. arXiv preprint arXiv:2212.09737 ,\\n2022. 4\\n[68] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\\nGit: A generative image-to-text transformer for vision and\\nlanguage. arXiv preprint arXiv:2205.14100 , 2022. 1\\n[69] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\\nGIT: A generative image-to-text transformer for vision and\\nlanguage. CoRR , 2022. 2, 7\\n[70] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\\nHongxia Yang. Ofa: Unifying architectures, tasks, and modal-\\nities through a simple sequence-to-sequence learning frame-\\nwork, 2022. 1, 2, 7\\n[71] Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao\\nZheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao,\\n11'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 11}, page_content='Ying Shan, et al. Caption anything: Interactive image de-\\nscription with diverse multimodal controls. arXiv preprint\\narXiv:2305.02677 , 2023. 4\\n[72] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\\nmodel pretraining with weak supervision. arXiv preprint\\narXiv:2108.10904 , 2021. 2\\n[73] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\\nmodel pretraining with weak supervision. In The Tenth In-\\nternational Conference on Learning Representations, ICLR\\n2022, Virtual Event, April 25-29, 2022 , 2022. 1\\n[74] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\\nQuoc V Le. Finetuned language models are zero-shot learners.\\narXiv preprint arXiv:2109.01652 , 2021. 3\\n[75] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 1492–1500,\\n2017. 4\\n[76] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\\nBengio. Show, attend and tell: Neural image caption gener-\\nation with visual attention. In International conference on\\nmachine learning , pages 2048–2057. PMLR, 2015. 2\\n[77] Linjie Yang, Kevin Tang, Jianchao Yang, and Li-Jia Li. Dense\\ncaptioning with joint inference and visual context. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 2193–2202, 2017. 2\\n[78] Zhilin Yang, Ye Yuan, Yuexin Wu, William W Cohen, and\\nRuss R Salakhutdinov. Review networks for caption genera-\\ntion. Advances in neural information processing systems , 29,\\n2016. 2\\n[79] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang,\\nand Xia Hu. Data-centric ai: Perspectives and challenges. In\\nProceedings of the 2023 SIAM International Conference on\\nData Mining (SDM) , pages 945–948. SIAM, 2023. 2, 3\\n[80] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan\\nYang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-\\ncentric artificial intelligence: A survey. arXiv preprint\\narXiv:2303.10158 , 2023. 2, 3\\n[81] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei\\nZhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl:\\nRevisiting visual representations in vision-language models.\\nInIEEE Conference on Computer Vision and Pattern Recog-\\nnition, CVPR 2021, virtual, June 19-25, 2021 , 2021. 1, 5\\n[82] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei\\nZhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl:\\nRevisiting visual representations in vision-language models.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 5579–5588, 2021. 2, 4\\n[83] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\\nformer language models. arXiv preprint arXiv:2205.01068 ,\\n2022. 7[84] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen,\\nWenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks, blip-\\n2 answers: Automatic questioning towards enriched visual\\ndescriptions. arXiv preprint arXiv:2303.06594 , 2023. 6\\n12'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 12}, page_content='A. F USECAPFused Captions\\nIn this section, we present supplementary details about\\nthe newly proposed fused dataset presented in Section 3. Ap-\\npendix A.2 presents examples of this enriched data, adding\\nto the ones shown in Figure 1.\\nA.1. Visual Experts Implementation\\nWe utilize the Visual Experts discussed in Section 3.1, in\\nthe following manner,\\n•Object detection We consider objects as valid detec-\\ntions if they surpass a pre-determined detection confi-\\ndence threshold of 0.7.\\n•Attribute Detection We incorporate attributes to each\\nvalid predicted object if the attribute confidence sur-\\npasses a 0.2 threshold.\\n•OCR : We use CRAFT and Parseq with default infer-\\nence parameters [5,8]. The text recognized is attributed\\nto the object that has the smallest bounding box encom-\\npassing it.\\nA.2. Training Set Generation for LLM Fuser with\\nChatGPT.\\nAs outlined in Section 3.2, we harness the zero-shot ca-\\npabilities of ChatGPT to generate a compact dataset encom-\\npassing 20,000 examples. This data is subsequently used\\nto fine-tune an open-source Large Language Model (LLM).\\nThe prompt provided to ChatGPT is as follows:\\n”A caption of an image is given: original caption .\\nThe following objects are detected in the image from left to\\nright:\\nAa1\\n1, ...,ak−1\\n1andak\\n1o1[with the following text: t1].\\n...\\nAa1\\nn, ...,akn−1\\nn andaknnon[with the following text: tn].\\nWrite a comprehensive and concise caption of the scene\\nusing the objects detected.”\\nOriginal: A man preparing to catch a frisbee in front of some houses.Ours: A man in white and blue shorts prepares to catch a white frisbee in front of a stone wall and a black metal fence, with a brown and red roof in the background, under a blue sky.Original: The Bug Club | Membership | Romney Hythe & Dymchurch RailwayOurs: A black and orange train, belonging to The Bug Club, sits on the tracks of Romney Hythe & Dymchurch Railway.\\nWe denote {oi}N\\ni=1as the set of objects detected, {aj\\ni}N\\ni=1\\nas the attributes related to each object, and {ti}N\\ni=1as texts\\nrelated to each object.\\nB. Training Settings\\nB.1. LLM Fuser\\nOur LLM Fuser is a fine-tuned FlanT5-XL [19] model.\\nWe used the huggingface library to fine-tune it on a sin-\\ngle NVIDIA A40 GPU. We trained the model for 4,000\\noptimization steps, with batch size of 32, a learning rate of\\n5·10−5, and a linear scheduling strategy. We limit the source\\nand target length to 100 and 200 tokens, respectively.\\nB.2. Caption Generator\\nFigure 5 offers further examples, supplementing those\\nfound in Figure 3 and providing additional instances of the\\nmodel’s outputs. These examples further emphasize the\\nability of the captioning model to generate semantically rich\\ncaptions.\\nTraining. The pre-training and subsequent fine-tuning of\\nthe captioning model, described in Section 5, was performed\\non eight NVIDIA A100 GPUs. For setting the pre-training\\nhyperparameters, we followed the approach outlined in the\\noriginal BLIP implementation [38], except for maximum\\ncaption length, batch size, and initial learning rate. We devi-\\nated from the original model’s batch size to accommodate\\nthe increased token length used in our implementation. To\\nmaintain stability during pre-training with a smaller batch\\nsize, we reduced the initial learning rate. The batch size used\\nfor pre-training was 400(50per GPU). The initial learning\\nrate used was 6·10−5.\\nMaximum Caption Length. The original BLIP model\\nimposes a training and inference limit of 30tokens per cap-\\ntion. With our enrichment process, however, caption lengths\\ntend to be longer and beyond this original threshold. Ac-\\ncordingly, we have increased the maximum caption length\\nto 60 tokens. This limit is maintained through both the pre-\\ntraining and fine-tuning stages of the caption generator, as\\nwell as during the actual generation of captions.\\n13'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 13}, page_content='Original: A hard black case sits outside a door.Ours: A hard black case sits outside a wood door with the text \"DESPARPAJO\" and a pink handle.\\nOriginal: rainbow lorikeet bird in treeOurs: A rainbow lorikeet bird perches on a tree amidst green leaves and blue sky, with its striking red body, blue head, and vibrant blue and orange beak.\\nOriginal: Hard times teach us valuable lessons. Handwriting on a napkin with a cup of coffee stock photosOurs: A cup of coffee sits on a red saucer, accompanied by a white napkin and a metal pen. The scene suggests a reminder that hard times teach valuable lessons, as captured in handwritten text.Original: happy kids sitting on the grass.Ours: A group of happy blond girls sitting on the grass, wearing red, yellow, and blue shirts, with one girl having a small nose and another having a closed eye.\\nOriginal: A man playing tennis in a tennis court Ours: A man in a white shirt and red shorts plays tennis on a court with a blue racket and white socks. He wears dark sunglasses and stands in front of a chain link fence and a green tree.\\nOriginal: A sign in my bed room.Ours: A black letter hangs on a white wall with the text \"Shack\" in my bedroom.Figure 4. Examples of the proposed F USECAPenriched-captions dataset.\\nBLIP2:a man and woman sitting on a motorcycle outsideOurs: a smiling man and woman pose for a picture in front of a red motorcycle, with the man wearing a blue shirt and black glasses, and the woman wearing a white shirt\\nOFA: a man and a woman sitting on a motorcycleGIT: a man and a woman posing for a picture.Prismer:A man and a woman sitting on a motorcycle.\\nOurs: a white cow rests in a lush green field under a clear blue sky, with a large green tree in the background and a dark shadow cast on the groundBLIP2: a cow is laying in the grass on a sunny dayOFA: a white cow sitting in a grassy fieldGIT: a cow laying in a field with a tree in the background.Prismer: A white cow laying on a lush green field.\\n14'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 14}, page_content='Ours: a man wearing a white shirt and blue jeans rides a motorbike in a parking lot surrounded by white and yellow tents, with a white line marking the edge of the parking\\nBLIP2: a man riding a motorcycle in a parking lot with tentsOFA: a man riding a motorcycle in a parking lotGIT: a man riding a small motorcycle in a parking lot.Prismer:A man riding a motorcycle in a parking lot.\\nOurs: a man wearing a red hat and blue jeans rides a white horse with a long tail, while a small white dog follows closely behind\\nBLIP2:a man riding a horse with a dog in the fieldOFA: a man riding on the back of a white horseGIT: a man riding a horse with a dog in the background.Prismer: A man riding a horse next to a small dog.\\nOurs: a woman with brown hair wearing a white shirt and black pants throws a white frisbee into a metal basket on a green grassy field\\nBLIP2: a woman is holding a frisbee in front of a metal cageOFA: a woman throwing a frisbee into a metal basketGIT: a woman is holding a frisbee in her hand.Prismer: woman throwing a frisbee into a metal cage.\\nBLIP2: a large elephant is riding in the back of a truckOurs: a brown elephant stands on the back of a truck on a gray road surrounded by green trees and grass under a cloudy sky, with a white line marking the edge of the roadOFA: an elephant is riding in the back of a truckGIT: an elephant is riding in the back of a truck.Prismer:An elephant is riding in the back of a truck.\\nOurs: a black bird soars over a lush green valley, with a brown building in the foreground and a white and blue sky overheadBLIP2: a bird flying over a green valley with mountains in the backgroundOFA: a bird flies over the alleghenymountainsGIT: a bird flying over a city with mountains in the background.Prismer: A bird flying over a lush green hillside.15'),\n",
       " Document(metadata={'source': '/home/user/Meet_Patel/new1/VectorDB_Pinecone_9/pdfs/FuseCap.pdf', 'page': 15}, page_content='Ours: a military helicopter with black blades and propellers flies through a gray sky, emitting orange and red smoke\\nBLIP2: a helicopter is flying through the airwith smoke coming out of the tailOFA: a helicopter flying in the skyGIT: a helicopter is flying through the airwith smoke coming out of it.Prismer: A helicopter flying through a gray sky with propellers.\\nOurs: a colorful assortment of vegetables, including carrots, celery, and lettuce, are arranged on a counter next to a white blender with a green handle\\nBLIP2:a pile of vegetables sitting on a counter next to a juicerOFA: a pile of vegetables sitting next to a white blenderGIT: a bunch of carrots and other vegetables on a counter.Prismer: A close upof a blender and vegetables on a table.\\nOFA: a woman holding an umbrella in front of a shopGIT: a woman holding an umbrella outside a shop.\\nOurs: a woman stands in front of a store, holding a white umbrella she wears blue jeans, a white scarf, and blue boots in the background, there is a wicker basketPrismer: A woman holding a white umbrella over her head.\\nBLIP2:a woman holding an umbrella outside a store on a cobblestone streetFigure 5. Comparative illustration of captions generated by our BLIP FUSECAPand other top-performing models.\\n16')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUSECAP: Leveraging Large Language Models\n",
      "for Enriched Fused Image Captions\n",
      "Noam Rotstein* David Bensa ¨ıd* Shaked Brody Roy Ganz Ron Kimmel\n",
      "Technion - Israel Institute of Technology\n",
      "*Indicates equal contribution.\n",
      "Abstract\n",
      "The advent of vision-language pre-training techniques en-\n",
      "hanced substantial progress in the development of models for\n",
      "image captioning. However, these models frequently produce\n",
      "generic captions and may omit semantically important im-\n",
      "age details. This limitation can be traced back to the image-\n",
      "text datasets; while their captions typically offer a general\n",
      "description of image content, they frequently omit salient\n",
      "details. Considering the magnitude of these datasets, man-\n",
      "ual reannotation is impractical, emphasizing the need for an\n",
      "automated approach. To address this challenge, we leverage\n",
      "existing captions and explore augmenting them with visual\n",
      "details using “frozen” vision experts including an object\n",
      "detector, an attribute recognizer, and an Optical Character\n",
      "Recognizer (OCR). Our proposed method, FUSECAP, fuses\n",
      "the outputs of such vision experts with the original captions\n",
      "using a large language model (LLM), yielding comprehen-\n",
      "sive image descriptions. We automatically curate a training\n",
      "set of 12M image-enriched caption pairs. These pairs un-\n",
      "dergo extensive evaluation through both quantitative and\n",
      "qualitative analyses. Subsequently, this data is utilized to\n",
      "train a captioning generation BLIP-based model. This model\n",
      "outperforms current state-of-the-art approaches, producing\n",
      "more precise and detailed descriptions, demonstrating the\n",
      "effectiveness of the proposed data-centric approach. We\n",
      "release this large-scale dataset of enriched image-caption\n",
      "pairs for the community.\n",
      "1. Introduction\n",
      "The generation of image captions that effectively cap-\n",
      "ture essential descriptive elements has been a longstanding\n",
      "goal in computer vision [7, 32, 34, 36, 49, 55]. In recent\n",
      "years, image captioning tasks [3, 44] have gained signifi-\n",
      "cant research attention and interest due to the success of\n",
      "Vision Language (VL) models. This achievement mainly\n",
      "stems from the ability to efficiently harness the massive\n",
      "amount of image-caption pairs accessible online, using Vi-\n",
      "Original: Two men with eye glasseslooking at somethingOurs: Two bespectacled men, one with black glasses and a black and brown beard, the other with silver glasses and short brown hair, sit together with an open blue laptop on a table in front of them. A graycat lounges nearbyOriginal: Mhmm, some clouds inthe sky Ours: A woman wearing dark sunglasses stands next to a red car with a black license plate reading 166882, PRI. The car has off and round headlights, a chrome and silver bumper, a black tire, and a red door. The cloudy and white sky is visible in the background.\n",
      "Original: save yourself the expense of a professional arrangement . Ours: Floral Arrangement: A colorful assortment of sunflowers, yellow, white, orange, and purple flowers, and green leaves arranged on a black and wood table.\n",
      "Ours: A woman with blond, long hair wearing a black belt and pants attends the premiere of The Little Stranger in 2018.Original: <PERSON> 2018 : <PERSON>: The Little Stranger Premiere -01Figure 1. FUSECAPcaptions. An illustration comparing our\n",
      "FUSECAPenriched captions with the original ground-truth captions\n",
      "before the fusing process. The examples are from COCO, SBU,\n",
      "CC, and CC12 datasets, displayed from top to bottom.\n",
      "sion Language Pre-training (VLP) [15, 40, 53], followed\n",
      "by task-specific fine-tuning. However, despite remarkable\n",
      "advancements in image captioning, current state-of-the-art\n",
      "models [14, 28, 37, 38, 68, 70, 73, 81] produce captions that\n",
      "often overlook key semantic elements. As images are rich\n",
      "1arXiv:2305.17718v2  [cs.CV]  15 Nov 2023\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUSECAP: Leveraging Large Language Models\n",
      "for Enriched Fused Image Captions\n",
      "Noam Rotstein* David Bensa ¨ıd* Shaked Brody Roy Ganz Ron Kimmel\n",
      "Technion - Israel Institute of Technology\n",
      "*Indicates equal contribution.\n",
      "Abstract\n",
      "The advent of vision-language pre-training techniques en-\n",
      "hanced substantial progress in the development of models for\n",
      "image captioning. However, these models frequently produce\n",
      "generic captions and may omit semantically important im-\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age details. This limitation can be traced back to the image-\n",
      "text datasets; while their captions typically offer a general\n",
      "description of image content, they frequently omit salient\n",
      "details. Considering the magnitude of these datasets, man-\n",
      "ual reannotation is impractical, emphasizing the need for an\n",
      "automated approach. To address this challenge, we leverage\n",
      "existing captions and explore augmenting them with visual\n",
      "details using “frozen” vision experts including an object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name google-bert/bert-base-uncased. Creating a new one with mean pooling.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/home/user/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"google-bert/bert-base-uncased\")\n",
    "embedding = embeddings_model.embed_query(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '105df79f-6673-4d4e-b944-b791a42783ef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pinecone.Pinecone(api_key = PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"testing\"\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creta Embedding for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.pinecone.Pinecone at 0x7f6c11b52ed0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Pinecone vector store\n",
    "vectorstore = Pinecone (index, embeddings_model.embed_query, text_key=\"page_content\")\n",
    "\n",
    "# Add documents to the vector store\n",
    "vectorstore.add_texts([t.page_content for t in text_chunks])\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Specifically, we leverage ChatGPT [11] to generate “fusing”\\nexamples which are then used to fine-tune a pre-trained Flan-\\nT5 model [54]. We apply this method to enrich captions\\nof a human-annotated dataset (COCO [44]) and large-scale\\ndatasets collected from the web (CC12 [12], CC [58], and\\nSBU [65]). This process produces an enriched collection\\nconsisting of 12M image-text pairs. To confirm the quality\\nof our generated dataset, we first show that humans favor'),\n",
       " Document(page_content='Specifically, we leverage ChatGPT [11] to generate “fusing”\\nexamples which are then used to fine-tune a pre-trained Flan-\\nT5 model [54]. We apply this method to enrich captions\\nof a human-annotated dataset (COCO [44]) and large-scale\\ndatasets collected from the web (CC12 [12], CC [58], and\\nSBU [65]). This process produces an enriched collection\\nconsisting of 12M image-text pairs. To confirm the quality\\nof our generated dataset, we first show that humans favor'),\n",
       " Document(page_content='Specifically, we leverage ChatGPT [11] to generate “fusing”\\nexamples which are then used to fine-tune a pre-trained Flan-\\nT5 model [54]. We apply this method to enrich captions\\nof a human-annotated dataset (COCO [44]) and large-scale\\ndatasets collected from the web (CC12 [12], CC [58], and\\nSBU [65]). This process produces an enriched collection\\nconsisting of 12M image-text pairs. To confirm the quality\\nof our generated dataset, we first show that humans favor'),\n",
       " Document(page_content='can be found in the appendix. After this phase, the fine-tuned\\nmodel can be used to generate an enriched captions dataset.\\n4. Training a Captioning Model\\nWe apply FUSECAP, presented in Section 3, to the COCO,\\nSBU, CC, and CC12 datasets, yielding 12 million aug-\\nmented image-caption pairs. Following our data-centric\\napproach, and to demonstrate the effectiveness of this aug-\\nmented dataset, we use it to train a captioning model based\\non the BLIP architecture [38] (Figure 2b). Adopting the')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve documents based on a query\n",
    "query = \"human-annotated dataset\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "docs\n",
    "# # Print the retrieved documents\n",
    "# for doc in docs:\n",
    "#     print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id = 'google-bert/bert-base-uncased', task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"dataset used in Fusecap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: FBeH2gFRBpklCoqU5IKfY)\n\nBad request:\nError in `inputs`: value is not a valid dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/distilbert-base-uncased-distilled-squad",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:598\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    599\u001b[0m         _output_key\n\u001b[1;32m    600\u001b[0m     ]\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    604\u001b[0m         _output_key\n\u001b[1;32m    605\u001b[0m     ]\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    379\u001b[0m }\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:153\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:603\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    599\u001b[0m         _output_key\n\u001b[1;32m    600\u001b[0m     ]\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    604\u001b[0m         _output_key\n\u001b[1;32m    605\u001b[0m     ]\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m     )\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    379\u001b[0m }\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/combine_documents/base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/combine_documents/stuff.py:257\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/llm.py:316\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    379\u001b[0m }\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:701\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    695\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    699\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    700\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:880\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    867\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    868\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m         )\n\u001b[1;32m    879\u001b[0m     ]\n\u001b[0;32m--> 880\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:738\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    737\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    739\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:725\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    717\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 725\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    733\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    734\u001b[0m         )\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1429\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1426\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1428\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1429\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1431\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1432\u001b[0m     )\n\u001b[1;32m   1433\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_community/llms/huggingface_hub.py:136\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    134\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 136\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:304\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:358\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    355\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m     )\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m:  (Request ID: FBeH2gFRBpklCoqU5IKfY)\n\nBad request:\nError in `inputs`: value is not a valid dict"
     ]
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: Lo-H-627SM-scvPp9uzVk)\n\nBad request:\nError in `inputs`: value is not a valid dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/distilbert-base-uncased-distilled-squad",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    379\u001b[0m }\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:153\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:603\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    599\u001b[0m         _output_key\n\u001b[1;32m    600\u001b[0m     ]\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    604\u001b[0m         _output_key\n\u001b[1;32m    605\u001b[0m     ]\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m     )\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    379\u001b[0m }\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/combine_documents/base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/combine_documents/stuff.py:257\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/llm.py:316\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     emit_warning()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    379\u001b[0m }\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:701\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    695\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    699\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    700\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:880\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    867\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    868\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m         )\n\u001b[1;32m    879\u001b[0m     ]\n\u001b[0;32m--> 880\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:738\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    737\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    739\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:725\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    717\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 725\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    733\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    734\u001b[0m         )\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1429\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1426\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1428\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1429\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1431\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1432\u001b[0m     )\n\u001b[1;32m   1433\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_community/llms/huggingface_hub.py:136\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    134\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 136\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:304\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:358\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    355\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m     )\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m:  (Request ID: Lo-H-627SM-scvPp9uzVk)\n\nBad request:\nError in `inputs`: value is not a valid dict"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "while True:\n",
    "    user_input = input(f\"Input Propt : \")\n",
    "    if user_input == \"exit\":\n",
    "        print('Exiting')\n",
    "        sys.exit()\n",
    "    if user_input == '':\n",
    "        continue\n",
    "    result = qa({'query': user_input})\n",
    "    print(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Create the retriever configuration dictionary\n",
    "# retriever_config = {\n",
    "#     \"type\": \"pinecone\",\n",
    "#     \"args\": {\n",
    "#         \"pinecone_index\": index,\n",
    "#         \"embedding_function\": embeddings_model.embed_query,\n",
    "#         \"text_key\": \"page_content\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "# # Create the RetrievalQA chain\n",
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever_config)\n",
    "\n",
    "# # Run the query\n",
    "# query = \"Fusecap outperforms which models\"\n",
    "# result = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# from uuid import uuid4\n",
    "\n",
    "# vector_store = PineconeVectorStore(index=index, embedding=embeddings_model)\n",
    "\n",
    "\n",
    "# uuids = [str(uuid4()) for _ in range(len(data))]\n",
    "# docsearch = vector_store.add_documents(documents=data, ids=uuids)\n",
    "# docsearch\n",
    "# # # Delete items from vector store\n",
    "# # vector_store.delete(ids=[uuids[-1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
