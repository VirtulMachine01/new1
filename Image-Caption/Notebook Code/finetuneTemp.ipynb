{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/new1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-01 14:45:18.324986: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-01 14:45:18.334984: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-01 14:45:18.347008: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-01 14:45:18.350515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-01 14:45:18.359876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-01 14:45:18.920642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Finetunnig for sentiment analysis \n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Chossing Base Model\n",
    "\n",
    "# model = 'distilbert-base-uncased' # smaller and faster BERT\n",
    "\n",
    "id2label = {0 : \"Negative\", 1 : \"Positive\"}\n",
    "label2id = {\"Negative\" : 0, \"Positive\" : 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2, id2label = id2label, label2id = label2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset for fine tuning\n",
    "\n",
    "dataset = load_dataset(\"shawhin/imdb-truncated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 9297.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess data\n",
    "\n",
    "# create tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', add_prefix_space = True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors = \"np\", truncation = True, max_length = 512)\n",
    "    return tokenized_inputs\n",
    "\n",
    "# adding pad token if not exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#tokenize training and validation dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Evaluation matrix\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "\n",
    "    return {\"accuracy\" : accuracy.compute(predictions=predictions, references=labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions : \n",
      "It was good.-Positive\n",
      "Not a fan, don't recommend.-Positive\n",
      "Better than the first one.-Positive\n",
      "This is not worth watching even once.-Positive\n",
      "This one is a pass.-Positive\n"
     ]
    }
   ],
   "source": [
    "# Performance before Fine tuning\n",
    "\n",
    "text_list = [\"It was good.\", \"Not a fan, don't recommend.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "\n",
    "print(\"Untrained model predictions : \")\n",
    "\n",
    "for text in text_list:\n",
    "    input = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    logits = model(input).logits\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \"-\" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    }
   ],
   "source": [
    "# fine tuning with LoRA\n",
    "\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification \n",
    "                         r=4, # intrinsic rank of trainable weight matrix\n",
    "                         lora_alpha=32, # this is like a learning rate\n",
    "                         lora_dropout=0.01, # probability of dropout\n",
    "                         target_modules=['q_lin']) # we apply lora to query layer\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/new1/.venv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "lr = 1e-3 # size of optimization step\n",
    "batch_size = 4 # number of examples processed per optimization step\n",
    "num_epoches = 10 # number of times model runs through training data\n",
    "\n",
    "#define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='distilbert-base-uncased' + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epoches,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 250/2500 [02:38<26:24,  1.42it/s]Trainer is attempting to log a value of \"{'accuracy': 0.879}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                  \n",
      " 10%|â–ˆ         | 250/2500 [04:04<26:24,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4187316298484802, 'eval_accuracy': {'accuracy': 0.879}, 'eval_runtime': 86.57, 'eval_samples_per_second': 11.551, 'eval_steps_per_second': 2.888, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 500/2500 [06:48<18:25,  1.81it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4281, 'grad_norm': 2.698070526123047, 'learning_rate': 0.0008, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.87}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                  \n",
      " 20%|â–ˆâ–ˆ        | 500/2500 [08:21<18:25,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4356079697608948, 'eval_accuracy': {'accuracy': 0.87}, 'eval_runtime': 93.1136, 'eval_samples_per_second': 10.74, 'eval_steps_per_second': 2.685, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 750/2500 [11:11<19:24,  1.50it/s]   Trainer is attempting to log a value of \"{'accuracy': 0.869}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                  \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 750/2500 [12:45<19:24,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.775346577167511, 'eval_accuracy': {'accuracy': 0.869}, 'eval_runtime': 93.2574, 'eval_samples_per_second': 10.723, 'eval_steps_per_second': 2.681, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1000/2500 [15:41<18:11,  1.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1928, 'grad_norm': 0.0014624304603785276, 'learning_rate': 0.0006, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.878}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1000/2500 [17:14<18:11,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6475468873977661, 'eval_accuracy': {'accuracy': 0.878}, 'eval_runtime': 92.2473, 'eval_samples_per_second': 10.84, 'eval_steps_per_second': 2.71, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [20:02<16:22,  1.27it/s]   Trainer is attempting to log a value of \"{'accuracy': 0.89}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [21:30<16:22,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7507137060165405, 'eval_accuracy': {'accuracy': 0.89}, 'eval_runtime': 88.2462, 'eval_samples_per_second': 11.332, 'eval_steps_per_second': 2.833, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1500/2500 [24:11<08:33,  1.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0589, 'grad_norm': 0.11405635625123978, 'learning_rate': 0.0004, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.88}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1500/2500 [25:37<08:33,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8473353385925293, 'eval_accuracy': {'accuracy': 0.88}, 'eval_runtime': 86.6664, 'eval_samples_per_second': 11.538, 'eval_steps_per_second': 2.885, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1750/2500 [28:22<08:11,  1.53it/s]  Trainer is attempting to log a value of \"{'accuracy': 0.879}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1750/2500 [29:49<08:11,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9633376598358154, 'eval_accuracy': {'accuracy': 0.879}, 'eval_runtime': 87.0062, 'eval_samples_per_second': 11.493, 'eval_steps_per_second': 2.873, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2000/2500 [32:37<05:20,  1.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0196, 'grad_norm': 4.849363540415652e-05, 'learning_rate': 0.0002, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.883}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2000/2500 [34:07<05:20,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9150023460388184, 'eval_accuracy': {'accuracy': 0.883}, 'eval_runtime': 90.3462, 'eval_samples_per_second': 11.069, 'eval_steps_per_second': 2.767, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2250/2500 [36:54<03:11,  1.31it/s]  Trainer is attempting to log a value of \"{'accuracy': 0.882}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2250/2500 [38:23<03:11,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9928166270256042, 'eval_accuracy': {'accuracy': 0.882}, 'eval_runtime': 89.0904, 'eval_samples_per_second': 11.225, 'eval_steps_per_second': 2.806, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [41:08<00:00,  1.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'grad_norm': 0.02723933756351471, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.884}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [42:39<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9991391897201538, 'eval_accuracy': {'accuracy': 0.884}, 'eval_runtime': 90.7632, 'eval_samples_per_second': 11.018, 'eval_steps_per_second': 2.754, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [42:40<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2560.4132, 'train_samples_per_second': 3.906, 'train_steps_per_second': 0.976, 'train_loss': 0.14040502071380614, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.14040502071380614, metrics={'train_runtime': 2560.4132, 'train_samples_per_second': 3.906, 'train_steps_per_second': 0.976, 'total_flos': 1112883852759936.0, 'train_loss': 0.14040502071380614, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer object to train the model\n",
    "\n",
    "trainer = Trainer(model=model, # peft model\n",
    "                  args=training_args, # hyperparameter\n",
    "                  train_dataset=tokenized_dataset[\"train\"], # training data\n",
    "                  eval_dataset=tokenized_dataset[\"validation\"], # validation data\n",
    "                  tokenizer=tokenizer, # tokenizer\n",
    "                  data_collator=data_collator, # dynamically pad examples in each batch\n",
    "                  compute_metrics=compute_metrics,) # evaluates model\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions : \n",
      "It was good.-Positive\n",
      "Not a fan, don't recommend.-Negative\n",
      "Better than the first one.-Positive\n",
      "This is not worth watching even once.-Positive\n",
      "This one is a pass.-Positive\n"
     ]
    }
   ],
   "source": [
    "# Performance After Fine tuning\n",
    "\n",
    "print(\"Trained model predictions : \")\n",
    "\n",
    "for text in text_list:\n",
    "    input = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    logits = model(input).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \"-\" + id2label[predictions.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
