{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Blip2Processor, Blip2ForConditionalGeneration, AutoProcessor, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "processor1 = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model1 =  BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "processor2 = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model2 =  BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "processor3 = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "model3 =  BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\")\n",
    "\n",
    "processor4 = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model4 = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "processor5 = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model5 = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"test2.jpg\").convert('RGB')\n",
    "\n",
    "inputs = processor1(img, return_tensors=\"pt\")\n",
    "out = model1.generate(**inputs)\n",
    "generated_text1 = processor1.decode(out[0], skip_special_tokens = True)\n",
    "\n",
    "inputs = processor2(img, return_tensors=\"pt\")\n",
    "out = model2.generate(**inputs)\n",
    "generated_text2 = processor2.decode(out[0], skip_special_tokens = True)\n",
    "\n",
    "inputs = processor3(img, return_tensors=\"pt\")\n",
    "out = model3.generate(**inputs)\n",
    "generated_text3 = processor3.decode(out[0], skip_special_tokens = True)\n",
    "\n",
    "inputs = processor4(img, return_tensors=\"pt\")\n",
    "out = model4.generate(**inputs)\n",
    "generated_text4 = processor4.decode(out[0], skip_special_tokens = True)\n",
    "\n",
    "inputs = processor5(img, return_tensors=\"pt\")\n",
    "out = model5.generate(**inputs)\n",
    "generated_text5 = processor5.decode(out[0], skip_special_tokens = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text1)\n",
    "print(generated_text2)\n",
    "print(generated_text3)\n",
    "print(generated_text4)\n",
    "print(generated_text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
