{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paddlepaddle paddleocr deep_tranlator PIL langdetect pytesseract\n",
    "from paddleocr import PaddleOCR\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image):\n",
    "    img_array = np.array(image)\n",
    "    print(\"Image Array Shape:\", img_array.shape)\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "    results = ocr.ocr(img_array, cls=True)\n",
    "    if results[0] is None:\n",
    "        return []\n",
    "    texts_with_positions = [line[1][0] for result in results for line in result]\n",
    "    return texts_with_positions\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    return GoogleTranslator(source='auto', target=dest_language).translate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Array Shape: (225, 225, 3)\n",
      "[2024/09/04 16:28:51] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/user/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/user/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/user/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/user/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.029481887817382812\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: cls num  : 2, elapsed : 0.007188320159912109\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: rec_res num  : 2, elapsed : 0.11164569854736328\n"
     ]
    }
   ],
   "source": [
    "file = \"/home/user/Meet_Patel/new1/OCR_documents/guj.png\"\n",
    "image = Image.open(file).convert('RGB')\n",
    "ocr_text = extract_text(image)\n",
    "translated_text = [translate_text(text) for text in ocr_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: ‡™ö‡´ã‡™∞, ‡™µ‡´ç‡™Ø‡™∏‡™®‡´Ä, ‡™™‡™æ‡™ñ‡™Ç‡™°‡´Ä, ‡™ï‡™æ‡™Æ‡´Ä ‡™§‡™•‡™æ ‡™ï‡™ø‡™Æ‡´Ä‡™ö‡™æ‡™ó‡™∞\n",
      "‡™Æ‡™æ‡™®‡™µ‡´ã‡™® ‡™∏‡™Ç‡™ó ‡™ï‡™∞‡™µ‡´ã ‡™®‡™π‡´Ä.\n",
      "\f\n",
      "Translated Text: Thieves, addicts, traitors, crooks and cheaters\n",
      "Do not associate with humans.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Tesseract to extract text\n",
    "    # Using Hindi ('hin') as a proxy language for OCR\n",
    "    extracted_text = pytesseract.image_to_string(gray, lang='guj')\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path):\n",
    "    # Extract text from image\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace this path with the actual path to your image\n",
    "    image_path = \"/home/user/Meet_Patel/new1/OCR_documents/guj.png\"\n",
    "    main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text (Hindi): ‡•§ ‡§≤‡•ã‡§¶‡•Ä‡§ï‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§≤, ‡§ï‡•Å‡§Å‡§ê ‡§î‡§∞ (\n",
      "‡§ï‡•á‡§≤‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ñ‡§ú‡§æ‡§®‡•á ‡§™‡§∞ ‡§Ö‡§à\n",
      "‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§¨‡§æ‡§¨‡§∞ ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§á‡§µ‡•ç‡§æ‡§π‡§ø‡§Æ ‡§ï‡•á\n",
      "_‡§ï‡§æ ‡§Ø‡§π‡•Ä‡§Ç ‡§∞‡§æ‡§ú‡•ç‡§Ø‡§æ‡§≠‡§ø‡§∑‡•á‡§ï ‡§π‡•Å‡§Ü‡•§ ‡•ß‡•¨‡•©‡•¨ ‡§Æ‡•á‡§Ç\n",
      "‡§¨‡§ö‡§æ‡§Ø‡§æ ‡§•‡§æ, ‡§â‡§∏ ‡§â‡§™‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§¶‡§≤‡•á ‡§π‡•Å‡§Æ‡§æ‡§É\n",
      "'‡§¢‡§ø‡§≤‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§Æ‡•á‡§Ç ‡§π‡•Å‡§Æ‡§æ‡§Ø‡•Ç‡§Å ‡§π‡§æ‡§∞ ‡§ó‡§Ø‡§æ‡•§ ‡§™‡•å‡§Å‡§ö ‡§¨‡§∞‡•ç‡§∑\n",
      "\f\n",
      "Translated Text: During the Lodhi period, the fort was given palaces, wells and a huge treasure. Babur was crowned here. In 1636, he was saved. In return for that favour, Humayun was defeated at Dhilgram. After five years, Humayun was defeated.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# List of supported languages and their respective Tesseract language codes\n",
    "supported_languages = {\n",
    "    'Hindi': 'hin',\n",
    "    'Gujarati': 'guj',\n",
    "    'Marathi': 'mar',\n",
    "    # Add more languages as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Tesseract to extract text\n",
    "    extracted_text = pytesseract.image_to_string(gray, lang=lang_code)\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path, language):\n",
    "    # Check if the language is supported\n",
    "    if language not in supported_languages:\n",
    "        print(f\"Language '{language}' is not supported.\")\n",
    "        return\n",
    "    \n",
    "    # Get the Tesseract language code\n",
    "    lang_code = supported_languages[language]\n",
    "    \n",
    "    # Extract text from image\n",
    "    extracted_text = extract_text_from_image(image_path, lang_code)\n",
    "    print(f\"Extracted Text ({language}):\", extracted_text)\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths and language with actual values\n",
    "    # image_path = \"path_to_your_image.jpg\"\n",
    "    # language = \"Gujarati\"  # or \"Hindi\", \"Marathi\", etc.\n",
    "    \n",
    "    # image_path = \"guj.png\"\n",
    "    image_path = \"hindi.png\"\n",
    "    language = \"Hindi\"\n",
    "    \n",
    "    main(image_path, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: hin\n",
      "Extracted Text: ‡•§ ‡§≤‡•ã‡§¶‡•Ä‡§ï‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§≤, ‡§ï‡•Å‡§Å‡§ê ‡§î‡§∞\n",
      "‡§ï‡•á‡§≤‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ñ‡§ú‡§æ‡§®‡•á ‡§™‡§∞\n",
      "‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§¨‡§æ‡§¨‡§∞ ‡§ï‡§ø‡§≤‡•á ‡§®‡•á‡§Ç ‡§á‡§µ‡•ç‡§æ‡§π‡§ø‡§∏ ‡§ï‡•á\n",
      "‡§ï‡§æ ‡§Ø‡§π‡•Ä‡§Ç ‡§∞‡§æ‡§ú‡•ç‡§£‡§Æ‡§ø‡§∑‡•á‡§ï ‡§π‡•Å‡§Ü‡•§ ‡•ß‡•©‡•©‡•¨ ‡§Æ‡•á‡§Ç\n",
      "‡§¨‡§ö‡§æ‡§Ø‡§æ ‡§•‡§æ, ‡§â‡§∏ ‡§â‡§™‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§¶‡§≤‡•á ‡§π‡•Å‡§Æ‡§æ‡§É\n",
      "'‡§°‡§ø‡§≤‡§ó‡•ç‡§∞‡§æ‡§∏ ‡§Æ‡•á‡§Ç ‡§π‡•Å‡§Æ‡§æ‡§Ø‡•Ç‡§Å ‡§π‡§æ‡§∞ ‡§ó‡§Ø‡§æ ‡•§ ‡§™‡§æ‡§Å‡§ö ‡§¨‡§∞‡•ç‡§∑\n",
      "Translated Text: In the Lodhi period, the fort had palaces, wells and forts and its vast treasure was given to Babur. Babur was crowned king here. In return for the favour of saving the fort in 1336, Humayun was defeated in the battle of Dilgras. After five years, he was made the king of the fort.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Mapping of detected languages to Tesseract language codes\n",
    "lang_map = {\n",
    "    'hi': 'hin',\n",
    "    'gu': 'guj',\n",
    "    'mr': 'mar',\n",
    "    'bn': 'ben',\n",
    "    'ta': 'tam',\n",
    "    'te': 'tel',\n",
    "    'kn': 'kan',\n",
    "    'ml': 'mal',\n",
    "    'pa': 'pan',\n",
    "    'or': 'ori',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code='eng'):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding to preprocess the image\n",
    "    threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    # Perform text extraction\n",
    "    config = f'--oem 3 --psm 6 -l {lang_code}'\n",
    "    text = pytesseract.image_to_string(threshold, config=config)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang_map.get(lang, 'eng')\n",
    "    except:\n",
    "        return 'eng'\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path):\n",
    "    # First, try to extract text using a general approach\n",
    "    initial_text = extract_text_from_image(image_path, 'eng+hin+guj+mar')\n",
    "    \n",
    "    # Detect the language of the extracted text\n",
    "    detected_lang_code = detect_language(initial_text)\n",
    "    \n",
    "    # Re-extract text using the detected language for better accuracy\n",
    "    if detected_lang_code != 'eng':\n",
    "        extracted_text = extract_text_from_image(image_path, detected_lang_code)\n",
    "    else:\n",
    "        extracted_text = initial_text\n",
    "    \n",
    "    print(f\"Detected Language: {detected_lang_code}\")\n",
    "    print(f\"Extracted Text: {extracted_text}\")\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"hindi.png\"\n",
    "    main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/swlh/document-verification-for-kyc-with-ai-ocr-computer-vision-tool-3485d85d75f6\n",
    "# https://github.com/shiva2410/Document_verification/tree/master\n",
    "# https://universe.roboflow.com/akash-k-p-gs9iu/aadhaar-card-details-extraction/model/3\n",
    "# https://surepass.io/aadhaar-card-ocr-api/#contact-form - demo\n",
    "# https://deepvue.tech/pan-card-ocr/#:~:text=The%20PAN%20Card%20OCR%20API%20takes%20scanned%20images%20or%20raw,right%20fields%20of%20the%20form. - pan demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43 üöÄ Python-3.12.4 torch-2.4.0+cu121 CUDA:0 (NVIDIA T400 4GB, 3901MiB)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /home/user/Meet_Patel/new1/OCR_documents/pan.jpeg: 416x640 1 dob, 1 father-s name, 1 name, 1 pan number, 34.7ms\n",
      "Speed: 0.7ms preprocess, 34.7ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.00000,  86.00000,  72.00000, 100.00000,   0.86915,   3.00000],\n",
      "        [ 15.00000,  64.00000, 169.00000,  82.00000,   0.73877,   1.00000],\n",
      "        [ 15.00000, 114.00000, 104.00000, 130.00000,   0.71968,   2.00000],\n",
      "        [219.00000, 108.00000, 274.00000, 160.00000,   0.45412,   0.00000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ultralyticsplus import YOLO, render_result\n",
    "\n",
    "# load model\n",
    "model = YOLO('foduucom/pan-card-detection')\n",
    "\n",
    "# set model parameters\n",
    "model.overrides['conf'] = 0.25  # NMS confidence threshold\n",
    "model.overrides['iou'] = 0.45  # NMS IoU threshold\n",
    "model.overrides['agnostic_nms'] = False  # NMS class-agnostic\n",
    "model.overrides['max_det'] = 1000  # maximum number of detections per image\n",
    "\n",
    "# set image\n",
    "image = 'pan.jpeg'\n",
    "\n",
    "# perform inference\n",
    "results = model.predict(image)\n",
    "\n",
    "# observe results\n",
    "print(results[0].boxes)\n",
    "render = render_result(model=model, image=image, result=results[0])\n",
    "render.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ultralytics YOLO <class 'ultralytics.yolo.engine.results.Boxes'> masks\n",
       " type: <class 'torch.Tensor'>\n",
       " shape: torch.Size([4, 6])\n",
       " dtype: torch.float32\n",
       "  + tensor([[ 14.00000,  86.00000,  72.00000, 100.00000,   0.86915,   3.00000],\n",
       "         [ 15.00000,  64.00000, 169.00000,  82.00000,   0.73877,   1.00000],\n",
       "         [ 15.00000, 114.00000, 104.00000, 130.00000,   0.71968,   2.00000],\n",
       "         [219.00000, 108.00000, 274.00000, 160.00000,   0.45412,   0.00000]], device='cuda:0')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Read image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply adaptive thresholding\n",
    "    processed_image = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    return processed_image\n",
    "\n",
    "def extract_text_from_image(image):\n",
    "    # Use pytesseract to do OCR on the image\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "def parse_aadhaar_text(text):\n",
    "    data = {}\n",
    "    # Regex patterns for extracting the fields\n",
    "    name_pattern = re.compile(r'\\bName\\b\\s*:\\s*([A-Za-z ]+)', re.IGNORECASE)\n",
    "    dob_pattern = re.compile(r'\\bDOB\\b\\s*:\\s*(\\d{2}/\\d{2}/\\d{4})', re.IGNORECASE)\n",
    "    gender_pattern = re.compile(r'\\bGender\\b\\s*:\\s*(Male|Female|Other)', re.IGNORECASE)\n",
    "    aadhaar_pattern = re.compile(r'\\b(\\d{4}\\s\\d{4}\\s\\d{4})\\b')\n",
    "\n",
    "    # Extract fields using regex\n",
    "    name_match = name_pattern.search(text)\n",
    "    dob_match = dob_pattern.search(text)\n",
    "    gender_match = gender_pattern.search(text)\n",
    "    aadhaar_match = aadhaar_pattern.search(text)\n",
    "\n",
    "    if name_match:\n",
    "        data['Name'] = name_match.group(1)\n",
    "    if dob_match:\n",
    "        data['DOB'] = dob_match.group(1)\n",
    "    if gender_match:\n",
    "        data['Gender'] = gender_match.group(1)\n",
    "    if aadhaar_match:\n",
    "        data['Aadhaar Number'] = aadhaar_match.group(1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def main(image_path):\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    extracted_text = extract_text_from_image(processed_image)\n",
    "    aadhaar_data = parse_aadhaar_text(extracted_text)\n",
    "    return aadhaar_data\n",
    "\n",
    "# Replace 'aadhaar_card.jpg' with the path to your Aadhaar card image\n",
    "image_path = 'addhar.jpeg'\n",
    "aadhaar_info = main(image_path)\n",
    "print(aadhaar_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
