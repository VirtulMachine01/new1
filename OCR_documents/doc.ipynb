{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paddlepaddle paddleocr deep_tranlator PIL langdetect pytesseract\n",
    "from paddleocr import PaddleOCR\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image):\n",
    "    img_array = np.array(image)\n",
    "    print(\"Image Array Shape:\", img_array.shape)\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "    results = ocr.ocr(img_array, cls=True)\n",
    "    if results[0] is None:\n",
    "        return []\n",
    "    texts_with_positions = [line[1][0] for result in results for line in result]\n",
    "    return texts_with_positions\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    return GoogleTranslator(source='auto', target=dest_language).translate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Array Shape: (225, 225, 3)\n",
      "[2024/09/04 16:28:51] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/user/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/user/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/user/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/user/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.029481887817382812\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: cls num  : 2, elapsed : 0.007188320159912109\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: rec_res num  : 2, elapsed : 0.11164569854736328\n"
     ]
    }
   ],
   "source": [
    "file = \"/home/user/Meet_Patel/new1/OCR_documents/guj.png\"\n",
    "image = Image.open(file).convert('RGB')\n",
    "ocr_text = extract_text(image)\n",
    "translated_text = [translate_text(text) for text in ocr_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: ‡™ö‡´ã‡™∞, ‡™µ‡´ç‡™Ø‡™∏‡™®‡´Ä, ‡™™‡™æ‡™ñ‡™Ç‡™°‡´Ä, ‡™ï‡™æ‡™Æ‡´Ä ‡™§‡™•‡™æ ‡™ï‡™ø‡™Æ‡´Ä‡™ö‡™æ‡™ó‡™∞\n",
      "‡™Æ‡™æ‡™®‡™µ‡´ã‡™® ‡™∏‡™Ç‡™ó ‡™ï‡™∞‡™µ‡´ã ‡™®‡™π‡´Ä.\n",
      "\f\n",
      "Translated Text: Thieves, addicts, traitors, crooks and cheaters\n",
      "Do not associate with humans.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Tesseract to extract text\n",
    "    # Using Hindi ('hin') as a proxy language for OCR\n",
    "    extracted_text = pytesseract.image_to_string(gray, lang='guj')\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path):\n",
    "    # Extract text from image\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace this path with the actual path to your image\n",
    "    image_path = \"/home/user/Meet_Patel/new1/OCR_documents/guj.png\"\n",
    "    main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text (Hindi): ‡•§ ‡§≤‡•ã‡§¶‡•Ä‡§ï‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§≤, ‡§ï‡•Å‡§Å‡§ê ‡§î‡§∞ (\n",
      "‡§ï‡•á‡§≤‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ñ‡§ú‡§æ‡§®‡•á ‡§™‡§∞ ‡§Ö‡§à\n",
      "‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§¨‡§æ‡§¨‡§∞ ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§á‡§µ‡•ç‡§æ‡§π‡§ø‡§Æ ‡§ï‡•á\n",
      "_‡§ï‡§æ ‡§Ø‡§π‡•Ä‡§Ç ‡§∞‡§æ‡§ú‡•ç‡§Ø‡§æ‡§≠‡§ø‡§∑‡•á‡§ï ‡§π‡•Å‡§Ü‡•§ ‡•ß‡•¨‡•©‡•¨ ‡§Æ‡•á‡§Ç\n",
      "‡§¨‡§ö‡§æ‡§Ø‡§æ ‡§•‡§æ, ‡§â‡§∏ ‡§â‡§™‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§¶‡§≤‡•á ‡§π‡•Å‡§Æ‡§æ‡§É\n",
      "'‡§¢‡§ø‡§≤‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§Æ‡•á‡§Ç ‡§π‡•Å‡§Æ‡§æ‡§Ø‡•Ç‡§Å ‡§π‡§æ‡§∞ ‡§ó‡§Ø‡§æ‡•§ ‡§™‡•å‡§Å‡§ö ‡§¨‡§∞‡•ç‡§∑\n",
      "\f\n",
      "Translated Text: During the Lodhi period, the fort was given palaces, wells and a huge treasure. Babur was crowned here. In 1636, he was saved. In return for that favour, Humayun was defeated at Dhilgram. After five years, Humayun was defeated.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# List of supported languages and their respective Tesseract language codes\n",
    "supported_languages = {\n",
    "    'Hindi': 'hin',\n",
    "    'Gujarati': 'guj',\n",
    "    'Marathi': 'mar',\n",
    "    # Add more languages as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Tesseract to extract text\n",
    "    extracted_text = pytesseract.image_to_string(gray, lang=lang_code)\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path, language):\n",
    "    # Check if the language is supported\n",
    "    if language not in supported_languages:\n",
    "        print(f\"Language '{language}' is not supported.\")\n",
    "        return\n",
    "    \n",
    "    # Get the Tesseract language code\n",
    "    lang_code = supported_languages[language]\n",
    "    \n",
    "    # Extract text from image\n",
    "    extracted_text = extract_text_from_image(image_path, lang_code)\n",
    "    print(f\"Extracted Text ({language}):\", extracted_text)\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths and language with actual values\n",
    "    # image_path = \"path_to_your_image.jpg\"\n",
    "    # language = \"Gujarati\"  # or \"Hindi\", \"Marathi\", etc.\n",
    "    \n",
    "    # image_path = \"guj.png\"\n",
    "    image_path = \"hindi.png\"\n",
    "    language = \"Hindi\"\n",
    "    \n",
    "    main(image_path, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: hin\n",
      "Extracted Text: ‡•§ ‡§≤‡•ã‡§¶‡•Ä‡§ï‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§≤, ‡§ï‡•Å‡§Å‡§ê ‡§î‡§∞\n",
      "‡§ï‡•á‡§≤‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ñ‡§ú‡§æ‡§®‡•á ‡§™‡§∞\n",
      "‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§¨‡§æ‡§¨‡§∞ ‡§ï‡§ø‡§≤‡•á ‡§®‡•á‡§Ç ‡§á‡§µ‡•ç‡§æ‡§π‡§ø‡§∏ ‡§ï‡•á\n",
      "‡§ï‡§æ ‡§Ø‡§π‡•Ä‡§Ç ‡§∞‡§æ‡§ú‡•ç‡§£‡§Æ‡§ø‡§∑‡•á‡§ï ‡§π‡•Å‡§Ü‡•§ ‡•ß‡•©‡•©‡•¨ ‡§Æ‡•á‡§Ç\n",
      "‡§¨‡§ö‡§æ‡§Ø‡§æ ‡§•‡§æ, ‡§â‡§∏ ‡§â‡§™‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§¶‡§≤‡•á ‡§π‡•Å‡§Æ‡§æ‡§É\n",
      "'‡§°‡§ø‡§≤‡§ó‡•ç‡§∞‡§æ‡§∏ ‡§Æ‡•á‡§Ç ‡§π‡•Å‡§Æ‡§æ‡§Ø‡•Ç‡§Å ‡§π‡§æ‡§∞ ‡§ó‡§Ø‡§æ ‡•§ ‡§™‡§æ‡§Å‡§ö ‡§¨‡§∞‡•ç‡§∑\n",
      "Translated Text: In the Lodhi period, the fort had palaces, wells and forts and its vast treasure was given to Babur. Babur was crowned king here. In return for the favour of saving the fort in 1336, Humayun was defeated in the battle of Dilgras. After five years, he was made the king of the fort.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Mapping of detected languages to Tesseract language codes\n",
    "lang_map = {\n",
    "    'hi': 'hin',\n",
    "    'gu': 'guj',\n",
    "    'mr': 'mar',\n",
    "    'bn': 'ben',\n",
    "    'ta': 'tam',\n",
    "    'te': 'tel',\n",
    "    'kn': 'kan',\n",
    "    'ml': 'mal',\n",
    "    'pa': 'pan',\n",
    "    'or': 'ori',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code='eng'):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding to preprocess the image\n",
    "    threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    # Perform text extraction\n",
    "    config = f'--oem 3 --psm 6 -l {lang_code}'\n",
    "    text = pytesseract.image_to_string(threshold, config=config)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang_map.get(lang, 'eng')\n",
    "    except:\n",
    "        return 'eng'\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path):\n",
    "    # First, try to extract text using a general approach\n",
    "    initial_text = extract_text_from_image(image_path, 'eng+hin+guj+mar')\n",
    "    \n",
    "    # Detect the language of the extracted text\n",
    "    detected_lang_code = detect_language(initial_text)\n",
    "    \n",
    "    # Re-extract text using the detected language for better accuracy\n",
    "    if detected_lang_code != 'eng':\n",
    "        extracted_text = extract_text_from_image(image_path, detected_lang_code)\n",
    "    else:\n",
    "        extracted_text = initial_text\n",
    "    \n",
    "    print(f\"Detected Language: {detected_lang_code}\")\n",
    "    print(f\"Extracted Text: {extracted_text}\")\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"hindi.png\"\n",
    "    main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/swlh/document-verification-for-kyc-with-ai-ocr-computer-vision-tool-3485d85d75f6\n",
    "# https://github.com/shiva2410/Document_verification/tree/master\n",
    "# https://universe.roboflow.com/akash-k-p-gs9iu/aadhaar-card-details-extraction/model/3\n",
    "# https://surepass.io/aadhaar-card-ocr-api/#contact-form - demo\n",
    "# https://deepvue.tech/pan-card-ocr/#:~:text=The%20PAN%20Card%20OCR%20API%20takes%20scanned%20images%20or%20raw,right%20fields%20of%20the%20form. - pan demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: guj\n",
      "Extracted Text: ‡™ó‡´ã‡™ü, ‡™™‡´ç‡™∏‡™∏‡™®‡´Ä, ‡™™‡™æ‡™ñ‡™Ç‡™ï‡´Ä, ‡™ò‡™æ‡™Æ‡´Ä ‡™§‡™£‡™æ ‡™ß‡™ø‡™Æ‡´Ä‡™Ø‡™ì‡™æ‡™Æ‡™æ‡™ü\n",
      "‡™Æ‡™æ‡™®‡™® ‡™∏‡™ó‡™æ ‡™ï‡™∞‡™µ‡´ã ‡™∏‡™£‡´Ä.\n",
      "Detected Language: hin\n",
      "Extracted Text: ‡•§ ‡§≤‡•ã‡§¶‡•Ä‡§ï‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§≤, ‡§ï‡•Å‡§Å‡§ê‡•á ‡§î‡§∞\n",
      "‡§ï‡•á‡§≤‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ñ‡§ú‡§æ‡§®‡•á ‡§™‡§∞ ‡§Ö‡•ë\n",
      "‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§¨‡§æ‡§¨‡§∞ ‡§ï‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§á‡§ï‡§º‡§æ‡§π‡§ø‡§Æ ‡§ï‡•á\n",
      "_‡§ï‡§æ ‡§Ø‡§π‡•Ä‡§Ç ‡§∞‡§æ‡§ú‡•ç‡§£‡§≠‡§ø‡§ß‡•á‡§ï ‡§π‡•Å‡§Ü‡•§ ‡•ß‡•©‡•©‡•¨ ‡§∏‡•á‡§Ç\n",
      "‡§¨‡§ö‡§æ‡§Ø‡§æ ‡§•‡§æ, ‡§â‡§∏ ‡§â‡§™‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§¶‡§≤‡•á ‡§π‡•Å‡§Æ‡§æ:\n",
      "'‡§µ‡§ø‡§≤‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§Æ‡•á‡§Ç ‡§π‡•Å‡§Æ‡§æ‡§Ø‡•Ç‡§Å ‡§π‡§æ‡§∞ ‡§ó‡§Ø‡§æ‡•§ ‡§™‡•å‡§Å‡§ö ‡§¨‡§∞‡•ç‡§∑\n",
      "Translated Text: The fort was built with the help of a fort, a palace, a well and a fort and its huge treasure were looted during the Lodhi period. Babur was crowned king of Iqahim in the fort. In 1336, he was saved from the fort. In return for that favour, Humayun was defeated at Vilgram. five years\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Mapping of detected languages to Tesseract language codes\n",
    "lang_map = {\n",
    "    'hi': 'hin',\n",
    "    'gu': 'guj',\n",
    "    'mr': 'mar',\n",
    "    'bn': 'ben',\n",
    "    'ta': 'tam',\n",
    "    'te': 'tel',\n",
    "    'kn': 'kan',\n",
    "    'ml': 'mal',\n",
    "    'pa': 'pan',\n",
    "    'or': 'ori',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code='eng'):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding to preprocess the image\n",
    "    threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Perform text extraction\n",
    "    config = f'--oem 3 --psm 6 -l {lang_code}'\n",
    "    text = pytesseract.image_to_string(threshold, config=config)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang_map.get(lang, 'eng')\n",
    "    except:\n",
    "        return 'eng'\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def pdf_to_images(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        image_path = f\"page_{page_num}.png\"\n",
    "        pix.save(image_path)\n",
    "        images.append(image_path)\n",
    "    \n",
    "    return images\n",
    "\n",
    "def main(file_path):\n",
    "    if file_path.lower().endswith('.pdf'):\n",
    "        images = pdf_to_images(file_path)\n",
    "    else:\n",
    "        images = [file_path]\n",
    "    \n",
    "    all_extracted_text = \"\"\n",
    "    \n",
    "    for image_path in images:\n",
    "        # First, try to extract text using a general approach\n",
    "        initial_text = extract_text_from_image(image_path, 'eng+hin+guj+mar')\n",
    "\n",
    "        # Detect the language of the extracted text\n",
    "        detected_lang_code = detect_language(initial_text)\n",
    "\n",
    "        # Re-extract text using the detected language for better accuracy\n",
    "        if detected_lang_code != 'eng':\n",
    "            extracted_text = extract_text_from_image(image_path, detected_lang_code)\n",
    "        else:\n",
    "            extracted_text = initial_text\n",
    "\n",
    "        print(f\"Detected Language: {detected_lang_code}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "\n",
    "        all_extracted_text += extracted_text + \"\\n\"\n",
    "\n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(all_extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"guj.pdf\"  # Change this to your PDF or image file path\n",
    "    main(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43 üöÄ Python-3.12.4 torch-2.4.0+cu121 CUDA:0 (NVIDIA T400 4GB, 3901MiB)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /home/user/Meet_Patel/new1/OCR_documents/pan.jpeg: 416x640 1 dob, 1 father-s name, 1 name, 1 pan number, 34.7ms\n",
      "Speed: 0.7ms preprocess, 34.7ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.00000,  86.00000,  72.00000, 100.00000,   0.86915,   3.00000],\n",
      "        [ 15.00000,  64.00000, 169.00000,  82.00000,   0.73877,   1.00000],\n",
      "        [ 15.00000, 114.00000, 104.00000, 130.00000,   0.71968,   2.00000],\n",
      "        [219.00000, 108.00000, 274.00000, 160.00000,   0.45412,   0.00000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ultralyticsplus import YOLO, render_result\n",
    "\n",
    "# load model\n",
    "model = YOLO('foduucom/pan-card-detection')\n",
    "\n",
    "# set model parameters\n",
    "model.overrides['conf'] = 0.25  # NMS confidence threshold\n",
    "model.overrides['iou'] = 0.45  # NMS IoU threshold\n",
    "model.overrides['agnostic_nms'] = False  # NMS class-agnostic\n",
    "model.overrides['max_det'] = 1000  # maximum number of detections per image\n",
    "\n",
    "# set image\n",
    "image = 'pan.jpeg'\n",
    "\n",
    "# perform inference\n",
    "results = model.predict(image)\n",
    "\n",
    "# observe results\n",
    "print(results[0].boxes)\n",
    "render = render_result(model=model, image=image, result=results[0])\n",
    "render.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ultralytics YOLO <class 'ultralytics.yolo.engine.results.Boxes'> masks\n",
       " type: <class 'torch.Tensor'>\n",
       " shape: torch.Size([4, 6])\n",
       " dtype: torch.float32\n",
       "  + tensor([[ 14.00000,  86.00000,  72.00000, 100.00000,   0.86915,   3.00000],\n",
       "         [ 15.00000,  64.00000, 169.00000,  82.00000,   0.73877,   1.00000],\n",
       "         [ 15.00000, 114.00000, 104.00000, 130.00000,   0.71968,   2.00000],\n",
       "         [219.00000, 108.00000, 274.00000, 160.00000,   0.45412,   0.00000]], device='cuda:0')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/05 16:56:17] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/user/.paddleocr/whl/det/ml/Multilingual_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/user/.paddleocr/whl/rec/devanagari/devanagari_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/user/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/paddleocr/ppocr/utils/dict/devanagari_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/user/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='hi', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/09/05 16:56:18] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.023699522018432617\n",
      "[2024/09/05 16:56:18] ppocr DEBUG: cls num  : 6, elapsed : 0.013955831527709961\n",
      "[2024/09/05 16:56:18] ppocr DEBUG: rec_res num  : 6, elapsed : 0.16326475143432617\n",
      "Extracted Text (PaddleOCR): ‡§ø‡§π‡•á‡§®‡§¶‡•Ä Hindi TO English ‡§Ö‡§Ç‡§ó‡•á‡•õ‡•Ä Trn tka\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import cv2\n",
    "\n",
    "# Initialize PaddleOCR with Hindi and English language support\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='hi')\n",
    "\n",
    "# Preprocess the image to improve OCR accuracy\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Binarization for better text detection\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Dilation to enhance text features\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
    "\n",
    "    return dilated\n",
    "\n",
    "# Extract text using PaddleOCR\n",
    "def extract_text_paddle(image_path):\n",
    "    # Preprocess the image\n",
    "    processed_image = preprocess_image(image_path)\n",
    "\n",
    "    # Save the processed image (optional step for debugging)\n",
    "    processed_image_path = 'processed_image.png'\n",
    "    cv2.imwrite(processed_image_path, processed_image)\n",
    "\n",
    "    # Run OCR on the processed image\n",
    "    result = ocr.ocr(processed_image_path)\n",
    "\n",
    "    # Extract the detected text\n",
    "    extracted_text = \"\"\n",
    "    for line in result:\n",
    "        for word_info in line:\n",
    "            extracted_text += word_info[1][0] + \" \"\n",
    "\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Main function to handle OCR and display text\n",
    "def main_paddle(image_path):\n",
    "    extracted_text = extract_text_paddle(image_path)\n",
    "    print(\"Extracted Text (PaddleOCR):\", extracted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"multilang_image.png\"\n",
    "    main_paddle(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
