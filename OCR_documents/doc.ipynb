{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paddlepaddle paddleocr deep_tranlator PIL langdetect pytesseract\n",
    "from paddleocr import PaddleOCR\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image):\n",
    "    img_array = np.array(image)\n",
    "    print(\"Image Array Shape:\", img_array.shape)\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "    results = ocr.ocr(img_array, cls=True)\n",
    "    if results[0] is None:\n",
    "        return []\n",
    "    texts_with_positions = [line[1][0] for result in results for line in result]\n",
    "    return texts_with_positions\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    return GoogleTranslator(source='auto', target=dest_language).translate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Array Shape: (225, 225, 3)\n",
      "[2024/09/04 16:28:51] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/user/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/user/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/user/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/user/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.029481887817382812\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: cls num  : 2, elapsed : 0.007188320159912109\n",
      "[2024/09/04 16:28:52] ppocr DEBUG: rec_res num  : 2, elapsed : 0.11164569854736328\n"
     ]
    }
   ],
   "source": [
    "file = \"/home/user/Meet_Patel/new1/OCR_documents/guj.png\"\n",
    "image = Image.open(file).convert('RGB')\n",
    "ocr_text = extract_text(image)\n",
    "translated_text = [translate_text(text) for text in ocr_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: à¤à¤¹à¤¿à¤¤à¥à¤¦à¥€ à¤®à¥ˆ à¤²à¤¿à¤–à¤¿à¥‡à¤¡à¥à¤Ÿà¥‚ à¤°à¤¿à¤•à¤¾à¤ˆà¤¸à¤•à¥‹ 7 à¤ƒ\n",
      "â€œà¤µà¤ˆ à¤¯à¤¾ à¤°à¤•à¥â€à¤¸à¤¿à¤² à¤®à¥‡ à¤•à¤¨à¥à¤¨à¤Ÿ à¤•à¤°à¤¾à¤° 77\n",
      "\n",
      " \n",
      "\f\n",
      "---------------------------------------------------------------------------------------------\n",
      "Translated Text: 7 of the record mentioned here:\n",
      "\n",
      "â€œThere is no agreement between us in this matter 77\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Tesseract to extract text\n",
    "    # Using Hindi ('hin') as a proxy language for OCR\n",
    "    extracted_text = pytesseract.image_to_string(gray, lang='hin')\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path):\n",
    "    # Extract text from image\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace this path with the actual path to your image\n",
    "    image_path = \"hindi2.jpeg\"\n",
    "    main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text (Hindi): à¥¤ à¤²à¥‹à¤¦à¥€à¤•à¤¾à¤² à¤®à¥‡à¤‚ à¤•à¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤®à¤¹à¤², à¤•à¥à¤à¤ à¤”à¤° (\n",
      "à¤•à¥‡à¤²à¥‡ à¤”à¤° à¤‰à¤¸à¤•à¥‡ à¤µà¤¿à¤¶à¤¾à¤² à¤–à¤œà¤¾à¤¨à¥‡ à¤ªà¤° à¤…à¤ˆ\n",
      "à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾à¥¤ à¤¬à¤¾à¤¬à¤° à¤•à¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤‡à¤µà¥à¤¾à¤¹à¤¿à¤® à¤•à¥‡\n",
      "_à¤•à¤¾ à¤¯à¤¹à¥€à¤‚ à¤°à¤¾à¤œà¥à¤¯à¤¾à¤­à¤¿à¤·à¥‡à¤• à¤¹à¥à¤†à¥¤ à¥§à¥¬à¥©à¥¬ à¤®à¥‡à¤‚\n",
      "à¤¬à¤šà¤¾à¤¯à¤¾ à¤¥à¤¾, à¤‰à¤¸ à¤‰à¤ªà¤•à¤¾à¤° à¤•à¥‡ à¤¬à¤¦à¤²à¥‡ à¤¹à¥à¤®à¤¾à¤ƒ\n",
      "'à¤¢à¤¿à¤²à¤—à¥à¤°à¤¾à¤® à¤®à¥‡à¤‚ à¤¹à¥à¤®à¤¾à¤¯à¥‚à¤ à¤¹à¤¾à¤° à¤—à¤¯à¤¾à¥¤ à¤ªà¥Œà¤à¤š à¤¬à¤°à¥à¤·\n",
      "\f\n",
      "Translated Text: During the Lodhi period, the fort was given palaces, wells and a huge treasure. Babur was crowned here. In 1636, he was saved. In return for that favour, Humayun was defeated at Dhilgram. After five years, Humayun was defeated.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# List of supported languages and their respective Tesseract language codes\n",
    "supported_languages = {\n",
    "    'Hindi': 'hin',\n",
    "    'Gujarati': 'guj',\n",
    "    'Marathi': 'mar',\n",
    "    # Add more languages as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Tesseract to extract text\n",
    "    extracted_text = pytesseract.image_to_string(gray, lang=lang_code)\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path, language):\n",
    "    # Check if the language is supported\n",
    "    if language not in supported_languages:\n",
    "        print(f\"Language '{language}' is not supported.\")\n",
    "        return\n",
    "    \n",
    "    # Get the Tesseract language code\n",
    "    lang_code = supported_languages[language]\n",
    "    \n",
    "    # Extract text from image\n",
    "    extracted_text = extract_text_from_image(image_path, lang_code)\n",
    "    print(f\"Extracted Text ({language}):\", extracted_text)\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths and language with actual values\n",
    "    # image_path = \"path_to_your_image.jpg\"\n",
    "    # language = \"Gujarati\"  # or \"Hindi\", \"Marathi\", etc.\n",
    "    \n",
    "    # image_path = \"guj.png\"\n",
    "    image_path = \"hindi.png\"\n",
    "    language = \"Hindi\"\n",
    "    \n",
    "    main(image_path, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: hin\n",
      "Extracted Text: à¥¤ à¤²à¥‹à¤¦à¥€à¤•à¤¾à¤² à¤®à¥‡à¤‚ à¤•à¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤®à¤¹à¤², à¤•à¥à¤à¤ à¤”à¤°\n",
      "à¤•à¥‡à¤²à¥‡ à¤”à¤° à¤‰à¤¸à¤•à¥‡ à¤µà¤¿à¤¶à¤¾à¤² à¤–à¤œà¤¾à¤¨à¥‡ à¤ªà¤°\n",
      "à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾à¥¤ à¤¬à¤¾à¤¬à¤° à¤•à¤¿à¤²à¥‡ à¤¨à¥‡à¤‚ à¤‡à¤µà¥à¤¾à¤¹à¤¿à¤¸ à¤•à¥‡\n",
      "à¤•à¤¾ à¤¯à¤¹à¥€à¤‚ à¤°à¤¾à¤œà¥à¤£à¤®à¤¿à¤·à¥‡à¤• à¤¹à¥à¤†à¥¤ à¥§à¥©à¥©à¥¬ à¤®à¥‡à¤‚\n",
      "à¤¬à¤šà¤¾à¤¯à¤¾ à¤¥à¤¾, à¤‰à¤¸ à¤‰à¤ªà¤•à¤¾à¤° à¤•à¥‡ à¤¬à¤¦à¤²à¥‡ à¤¹à¥à¤®à¤¾à¤ƒ\n",
      "'à¤¡à¤¿à¤²à¤—à¥à¤°à¤¾à¤¸ à¤®à¥‡à¤‚ à¤¹à¥à¤®à¤¾à¤¯à¥‚à¤ à¤¹à¤¾à¤° à¤—à¤¯à¤¾ à¥¤ à¤ªà¤¾à¤à¤š à¤¬à¤°à¥à¤·\n",
      "Translated Text: In the Lodhi period, the fort had palaces, wells and forts and its vast treasure was given to Babur. Babur was crowned king here. In return for the favour of saving the fort in 1336, Humayun was defeated in the battle of Dilgras. After five years, he was made the king of the fort.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Mapping of detected languages to Tesseract language codes\n",
    "lang_map = {\n",
    "    'hi': 'hin',\n",
    "    'gu': 'guj',\n",
    "    'mr': 'mar',\n",
    "    'bn': 'ben',\n",
    "    'ta': 'tam',\n",
    "    'te': 'tel',\n",
    "    'kn': 'kan',\n",
    "    'ml': 'mal',\n",
    "    'pa': 'pan',\n",
    "    'or': 'ori',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code='eng'):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding to preprocess the image\n",
    "    threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    # Perform text extraction\n",
    "    config = f'--oem 3 --psm 6 -l {lang_code}'\n",
    "    text = pytesseract.image_to_string(threshold, config=config)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang_map.get(lang, 'eng')\n",
    "    except:\n",
    "        return 'eng'\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def main(image_path):\n",
    "    # First, try to extract text using a general approach\n",
    "    initial_text = extract_text_from_image(image_path, 'eng+hin+guj+mar')\n",
    "    \n",
    "    # Detect the language of the extracted text\n",
    "    detected_lang_code = detect_language(initial_text)\n",
    "    \n",
    "    # Re-extract text using the detected language for better accuracy\n",
    "    if detected_lang_code != 'eng':\n",
    "        extracted_text = extract_text_from_image(image_path, detected_lang_code)\n",
    "    else:\n",
    "        extracted_text = initial_text\n",
    "    \n",
    "    print(f\"Detected Language: {detected_lang_code}\")\n",
    "    print(f\"Extracted Text: {extracted_text}\")\n",
    "    \n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"hindi.png\"\n",
    "    main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/swlh/document-verification-for-kyc-with-ai-ocr-computer-vision-tool-3485d85d75f6\n",
    "# https://github.com/shiva2410/Document_verification/tree/master\n",
    "# https://universe.roboflow.com/akash-k-p-gs9iu/aadhaar-card-details-extraction/model/3\n",
    "# https://surepass.io/aadhaar-card-ocr-api/#contact-form - demo\n",
    "# https://deepvue.tech/pan-card-ocr/#:~:text=The%20PAN%20Card%20OCR%20API%20takes%20scanned%20images%20or%20raw,right%20fields%20of%20the%20form. - pan demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: hin\n",
      "Extracted Text: à¤¶à¥ à¤•à¤• à¥¦ à¥Œ+à¥¤ 7\n",
      "à¤œà¤¼à¤¿à¤•à¥à¤•à¤¾ à¤‡à¤•\n",
      "\n",
      "à¤¨à¤¾à¤® / à¥¬8à¥¥6:\n",
      "\n",
      "/(38à¥¥ ((à¤ªà¥à¤°à¤¾à¤•à¥à¤·à¤¾\n",
      "\n",
      "à¤œà¤¨à¥à¤® à¤¤à¤¾à¤°à¥€à¤– / 008: 7/06/995\n",
      "\n",
      "à¤ªà¥à¤°à¥‚à¤· / |/à¥­à¥¤à¥¥à¥¯\n",
      "\n",
      "846550732429\n",
      "à¤†à¤§à¤¾à¤° - à¤†à¤¦à¤®à¥€ à¤•à¤¾ à¤…à¤§à¤•à¤¿à¤°\n",
      "Translated Text: Sh k k 0 o +. 7\n",
      "Zikka Ek\n",
      "\n",
      "Name / 68à¥¥6:\n",
      "\n",
      "/(38à¥¥ ((Praaksha\n",
      "\n",
      "Date of Birth / 008: 7/06/995\n",
      "\n",
      "Male / |/7.à¥¥9\n",
      "\n",
      "846550732429\n",
      "Aadhaar - Man's Right\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Mapping of detected languages to Tesseract language codes\n",
    "lang_map = {\n",
    "    'hi': 'hin',\n",
    "    'gu': 'guj',\n",
    "    'mr': 'mar',\n",
    "    'bn': 'ben',\n",
    "    'ta': 'tam',\n",
    "    'te': 'tel',\n",
    "    'kn': 'kan',\n",
    "    'ml': 'mal',\n",
    "    'pa': 'pan',\n",
    "    'or': 'ori',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def extract_text_from_image(image_path, lang_code='eng'):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding to preprocess the image\n",
    "    threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Perform text extraction\n",
    "    config = f'--oem 3 --psm 6 -l {lang_code}'\n",
    "    text = pytesseract.image_to_string(threshold, config=config)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang_map.get(lang, 'eng')\n",
    "    except:\n",
    "        return 'eng'\n",
    "\n",
    "def translate_text(text, dest_language='en'):\n",
    "    translator = GoogleTranslator(source='auto', target=dest_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "def pdf_to_images(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        image_path = f\"page_{page_num}.png\"\n",
    "        pix.save(image_path)\n",
    "        images.append(image_path)\n",
    "    \n",
    "    return images\n",
    "\n",
    "def main(file_path):\n",
    "    if file_path.lower().endswith('.pdf'):\n",
    "        images = pdf_to_images(file_path)\n",
    "    else:\n",
    "        images = [file_path]\n",
    "    \n",
    "    all_extracted_text = \"\"\n",
    "    \n",
    "    for image_path in images:\n",
    "        # First, try to extract text using a general approach\n",
    "        initial_text = extract_text_from_image(image_path, 'eng+hin+guj+mar')\n",
    "\n",
    "        # Detect the language of the extracted text\n",
    "        detected_lang_code = detect_language(initial_text)\n",
    "\n",
    "        # Re-extract text using the detected language for better accuracy\n",
    "        if detected_lang_code != 'eng':\n",
    "            extracted_text = extract_text_from_image(image_path, detected_lang_code)\n",
    "        else:\n",
    "            extracted_text = initial_text\n",
    "\n",
    "        print(f\"Detected Language: {detected_lang_code}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "\n",
    "        all_extracted_text += extracted_text + \"\\n\"\n",
    "\n",
    "    # Translate the extracted text into English\n",
    "    translated_text = translate_text(all_extracted_text, 'en')\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"sample_image3.jpeg\"  # Change this to your PDF or image file path\n",
    "    main(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43 ğŸš€ Python-3.12.4 torch-2.4.0+cu121 CUDA:0 (NVIDIA T400 4GB, 3901MiB)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /home/user/Meet_Patel/new1/OCR_documents/pan.jpeg: 416x640 1 dob, 1 father-s name, 1 name, 1 pan number, 34.7ms\n",
      "Speed: 0.7ms preprocess, 34.7ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.00000,  86.00000,  72.00000, 100.00000,   0.86915,   3.00000],\n",
      "        [ 15.00000,  64.00000, 169.00000,  82.00000,   0.73877,   1.00000],\n",
      "        [ 15.00000, 114.00000, 104.00000, 130.00000,   0.71968,   2.00000],\n",
      "        [219.00000, 108.00000, 274.00000, 160.00000,   0.45412,   0.00000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ultralyticsplus import YOLO, render_result\n",
    "\n",
    "# load model\n",
    "model = YOLO('foduucom/pan-card-detection')\n",
    "\n",
    "# set model parameters\n",
    "model.overrides['conf'] = 0.25  # NMS confidence threshold\n",
    "model.overrides['iou'] = 0.45  # NMS IoU threshold\n",
    "model.overrides['agnostic_nms'] = False  # NMS class-agnostic\n",
    "model.overrides['max_det'] = 1000  # maximum number of detections per image\n",
    "\n",
    "# set image\n",
    "image = 'pan.jpeg'\n",
    "\n",
    "# perform inference\n",
    "results = model.predict(image)\n",
    "\n",
    "# observe results\n",
    "print(results[0].boxes)\n",
    "render = render_result(model=model, image=image, result=results[0])\n",
    "render.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ultralytics YOLO <class 'ultralytics.yolo.engine.results.Boxes'> masks\n",
       " type: <class 'torch.Tensor'>\n",
       " shape: torch.Size([4, 6])\n",
       " dtype: torch.float32\n",
       "  + tensor([[ 14.00000,  86.00000,  72.00000, 100.00000,   0.86915,   3.00000],\n",
       "         [ 15.00000,  64.00000, 169.00000,  82.00000,   0.73877,   1.00000],\n",
       "         [ 15.00000, 114.00000, 104.00000, 130.00000,   0.71968,   2.00000],\n",
       "         [219.00000, 108.00000, 274.00000, 160.00000,   0.45412,   0.00000]], device='cuda:0')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/11 16:34:23] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/user/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/user/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/user/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/user/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/09/11 16:34:24] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.02618241310119629\n",
      "[2024/09/11 16:34:24] ppocr DEBUG: cls num  : 3, elapsed : 0.026842355728149414\n",
      "[2024/09/11 16:34:24] ppocr DEBUG: rec_res num  : 3, elapsed : 0.18497061729431152\n",
      "Extracted Text (PaddleOCR): will tranribe any pondwin pelis. Acrun-shetw. Lat iates statoms yass diaws tte\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import cv2\n",
    "\n",
    "# Initialize PaddleOCR with Hindi and English language support\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# Preprocess the image to improve OCR accuracy\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Binarization for better text detection\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Dilation to enhance text features\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
    "\n",
    "    return dilated\n",
    "\n",
    "# Extract text using PaddleOCR\n",
    "def extract_text_paddle(image_path):\n",
    "    # Preprocess the image\n",
    "    processed_image = preprocess_image(image_path)\n",
    "\n",
    "    # Save the processed image (optional step for debugging)\n",
    "    processed_image_path = 'processed_image.png'\n",
    "    cv2.imwrite(processed_image_path, processed_image)\n",
    "\n",
    "    # Run OCR on the processed image\n",
    "    result = ocr.ocr(processed_image_path)\n",
    "\n",
    "    # Extract the detected text\n",
    "    extracted_text = \"\"\n",
    "    for line in result:\n",
    "        for word_info in line:\n",
    "            extracted_text += word_info[1][0] + \" \"\n",
    "\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Main function to handle OCR and display text\n",
    "def main_paddle(image_path):\n",
    "    extracted_text = extract_text_paddle(image_path)\n",
    "    print(\"Extracted Text (PaddleOCR):\", extracted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"enghand.jpeg\"\n",
    "    main_paddle(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data: {'à¥¤': {'bounding_box': (545, 0, 45, 384)}, 'à¥¬à¥ªà¥ªà¥¦à¥ª': {'bounding_box': (308, 347, 110, 41)}, 'à¥¬à¥ªà¥ªà¥«à¥¦': {'bounding_box': (707, 533, 140, 39)}, 'à¥ªà¥¦à¥ªà¥ªà¥ªà¥­': {'bounding_box': (566, 804, 102, 31)}, 'à¥¦à¥«': {'bounding_box': (440, 885, 21, 15)}, 'à¤¹à¥¨à¥¦': {'bounding_box': (475, 884, 50, 16)}, 'à¤²à¥‹': {'bounding_box': (509, 865, 27, 35)}, 'à¤•à¤•': {'bounding_box': (535, 865, 34, 35)}, 'à¥§à¥«à¥¦à¥¦': {'bounding_box': (571, 877, 60, 23)}, 'à¤¹à¥ˆ': {'bounding_box': (949, 872, 25, 28)}}\n",
      "Full Extracted Text:       \n",
      "\n",
      "GO KARNATAKA\n",
      "à²œà²¨à²¨ à²®à²¤à³à²¤à³ à²®à²°à²£à²—à³à²°à³†à²³à³† à²®à³à²–à³à²¯ à²°à²¿à²œà²¿à²¸à³à²Ÿà²¾à²°à²°à³ /.\n",
      "Chief Registrar af Birth and Death |\n",
      "stud 30.28 (GRE Abed deca) ||!\n",
      "â€˜| Form No. (See Rule 8) à²¸à²¿\n",
      "à²œà²¨à²¨ à²ªà³à²°à²®à²¾ à²ªà³à²°à²¾\n",
      "( 11/17 à²¨à³‡ à²ªà³à²°à²•à²°à²£ à²®à³‡à²°à³†à²—à³† à²•à³Šà³‚à²¡à²°à²¾à²¦)\n",
      "BIRTH CERTIFICATE\n",
      "\n",
      "(Issued Under Section 12 / 17)\n",
      "\n",
      "   \n",
      "\n",
      "à²œà²¿à²²à³à²²à³†à²¯, à²¬à³†à²‚à²—à²³à³‚à²°à³ à²®à²¹à²¾à²¨à²—à²° à²ªà²¾à²²à²¿à²•à³†à²¯\n",
      "\n",
      "à²ˆ à²•à³†à²³à²•à²‚à²¡ à²µà²¿à²µà²°à²£à³†à²¯à²¨à³à²¨à³ à²•à²°à³à²¨à²¾à²Ÿà²• à²°à²¾à²œà³à²¯à²¦\n",
      "à²¤à²¿à²—à³†à²—à³à²•à³Šà²‚à²²à²‰à³à²—à²¿à²—à³†à²—à²‚à²¦à²—à³ à²—à²®à²¾à²£à³€à²•à²°à²¿à²¸à²²à²¾à²—à²¿à²¦à³†.\n",
      "\n",
      "Pet \\ staat eyetel Png \\ arom adn icy\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter\n",
    "from deep_translator import GoogleTranslator\n",
    "import re\n",
    "\n",
    "# Set the Tesseract executable path if necessary\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('L')  # Convert to grayscale\n",
    "    img = img.filter(ImageFilter.SHARPEN)  # Sharpen the image\n",
    "    img = ImageEnhance.Contrast(img).enhance(2)  # Increase contrast\n",
    "    return img\n",
    "\n",
    "def extract_text_with_bounding_box(image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    \n",
    "    # Use Tesseract to do OCR on the image, specifying both English and Hindi languages\n",
    "    data = pytesseract.image_to_data(img, lang='eng+hin', output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    name_patterns = [\n",
    "        r'^[A-Z][a-z]+ [A-Z][a-z]+$',  # Simple \"First Last\" pattern\n",
    "        r'^[A-Z][a-z]+ [A-Z]\\. [A-Z][a-z]+$',  # \"First M. Last\" pattern\n",
    "        r'^[A-Z][a-z]+ [A-Z][a-z]+-[A-Z][a-z]+$',  # Hyphenated last name\n",
    "        r'^[\\u0900-\\u097F]+$',  # Hindi text pattern\n",
    "        # Add more patterns as needed\n",
    "    ]\n",
    "    \n",
    "    extracted_data = {}\n",
    "    n_boxes = len(data['level'])\n",
    "    for i in range(n_boxes):\n",
    "        text = data['text'][i].strip()\n",
    "        for pattern in name_patterns:\n",
    "            if re.match(pattern, text):\n",
    "                x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=2)\n",
    "                extracted_data[text] = {'bounding_box': (x, y, w, h)}\n",
    "                break\n",
    "    \n",
    "    full_text = pytesseract.image_to_string(img, lang='eng+kan')\n",
    "    return img, extracted_data, full_text\n",
    "\n",
    "def translate_text(text):\n",
    "    translator = GoogleTranslator(source='auto', target='en')\n",
    "    if text.strip() == \"\":\n",
    "        return \"\"\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "# Usage\n",
    "image_path = 'kannad2.jpeg'\n",
    "image_with_box, extracted_data, full_text = extract_text_with_bounding_box(image_path)\n",
    "\n",
    "# Display the image with the bounding box\n",
    "image_with_box.show()\n",
    "\n",
    "# Translate the extracted text\n",
    "translated_text = translate_text(full_text)\n",
    "\n",
    "# Print extracted and translated data\n",
    "print(\"Extracted Data:\", extracted_data)\n",
    "print(\"Full Extracted Text:\", full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Translated Text: a No.5 (See Rule 8)\n",
      "Form no. Cult Rule 8)\n",
      "Born Baw Pra\n",
      "\n",
      "(According to the 117th case a\n",
      "BIRTH CERTIFICATE\n",
      "(Issued under Section 12 / 17)\n",
      "\n",
      "   \n",
      "\n",
      "The following explanation is given in Karnataka fe 010003 2039, 016069 Â«222210 t25â‚¹05)\n",
      "\n",
      "erie' edt ibashui saka rra still bak tigegukomlaugigegandagu standardized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"Translated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data: {'box_0': {'text': 'àª¨à«ˆàª•à«àª¸à«àª¤à«\\n\\x0c', 'bounding_box': (1080, 930, 160, 55)}, 'box_1': {'text': '[àª¸àª¤àª•àª¡àª®àª¾à«¨-\\nàªà«‡\\n\\x0c', 'bounding_box': (510, 1150, 400, 160)}}\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageDraw\n",
    "\n",
    "# Preprocess the image (grayscale, sharpen, contrast enhancement)\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('L')  # Convert to grayscale\n",
    "    img = img.filter(ImageFilter.SHARPEN)  # Sharpen the image\n",
    "    img = ImageEnhance.Contrast(img).enhance(2)  # Increase contrast\n",
    "    return img\n",
    "\n",
    "# Function to manually define bounding boxes and extract text\n",
    "def extract_text_with_manual_boxes(image_path, boxes):\n",
    "    img = preprocess_image(image_path)\n",
    "\n",
    "    extracted_data = {}\n",
    "\n",
    "    for box_id, (x, y, w, h) in enumerate(boxes):\n",
    "        # Crop the image to the specified bounding box\n",
    "        cropped_img = img.crop((x, y, x + w, y + h))\n",
    "        # cropped_img.show()\n",
    "        # Extract text from the cropped region using Tesseract\n",
    "        extracted_text = pytesseract.image_to_string(cropped_img, lang='guj')\n",
    "        \n",
    "        # Draw the bounding box on the original image (for visualization)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=2)\n",
    "        \n",
    "        extracted_data[f\"box_{box_id}\"] = {'text': extracted_text, 'bounding_box': (x, y, w, h)}\n",
    "\n",
    "    return img, extracted_data\n",
    "\n",
    "# Define manual bounding boxes (x, y, width, height)\n",
    "# Replace these values with actual coordinates for your image\n",
    "manual_boxes = [\n",
    "    (1080, 930, 160, 55),   # Example box 1\n",
    "    (510, 1150, 400, 160),  # Example box 2\n",
    "    # Add more boxes based on regions of interest\n",
    "]\n",
    "\n",
    "# Process the image and extract data from the manually defined boxes\n",
    "image_path = 'meet.jpeg'  # Replace with your actual image file path\n",
    "image_with_boxes, extracted_data = extract_text_with_manual_boxes(image_path, manual_boxes)\n",
    "\n",
    "# Display the image with the drawn bounding boxes\n",
    "image_with_boxes.show()\n",
    "\n",
    "# Print the extracted data\n",
    "print(\"Extracted Data:\", extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1àª¸àª¤àª¡à«àª®àª¾à«¨\\n-2'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data['box_1']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Function to translate text using Google Translator\n",
    "def translate_to_english(text):\n",
    "    translator = GoogleTranslator(source='auto', target='en')\n",
    "    translated_text = translator.translate(text)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "purush = translate_text(extracted_data['box_1']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Male'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corrupt_msg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, extracted_data\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Initialize EasyOCR reader for Gujarati\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43measyocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Define manual bounding boxes (x, y, width, height)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Example boxes; replace with actual coordinates\u001b[39;00m\n\u001b[1;32m     56\u001b[0m manual_boxes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     57\u001b[0m     (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m50\u001b[39m),  \u001b[38;5;66;03m# Example box 1\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     (\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m75\u001b[39m),  \u001b[38;5;66;03m# Example box 2\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Add more boxes based on regions of interest\u001b[39;00m\n\u001b[1;32m     60\u001b[0m ]\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/easyocr/easyocr.py:184\u001b[0m, in \u001b[0;36mReader.__init__\u001b[0;34m(self, lang_list, gpu, model_storage_directory, user_network_directory, detect_network, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_enabled:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMD5 mismatch for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and downloads disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m model_path)\n\u001b[0;32m--> 184\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mwarning(\u001b[43mcorrupt_msg\u001b[49m)\n\u001b[1;32m    185\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(model_path)\n\u001b[1;32m    186\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRe-downloading the recognition model, please wait. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    187\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis may take several minutes depending upon your network connection.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corrupt_msg' is not defined"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageDraw\n",
    "import io\n",
    "\n",
    "# Preprocess the image (grayscale, sharpen, contrast enhancement)\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('L')  # Convert to grayscale\n",
    "    img = img.filter(ImageFilter.SHARPEN)  # Sharpen the image\n",
    "    img = ImageEnhance.Contrast(img).enhance(2)  # Increase contrast\n",
    "    return img\n",
    "\n",
    "# Convert PIL Image to numpy array\n",
    "def pil_to_numpy(img):\n",
    "    return np.array(img)\n",
    "\n",
    "# Convert numpy array to bytes\n",
    "def numpy_to_bytes(arr):\n",
    "    img = Image.fromarray(arr)\n",
    "    with io.BytesIO() as buf:\n",
    "        img.save(buf, format='PNG')\n",
    "        return buf.getvalue()\n",
    "\n",
    "# Function to manually define bounding boxes and extract text using EasyOCR\n",
    "def extract_text_with_manual_boxes(image_path, boxes, reader):\n",
    "    img = preprocess_image(image_path)\n",
    "    img_np = pil_to_numpy(img)  # Convert PIL image to numpy array\n",
    "\n",
    "    extracted_data = {}\n",
    "\n",
    "    for box_id, (x, y, w, h) in enumerate(boxes):\n",
    "        # Crop the image to the specified bounding box\n",
    "        cropped_img_np = img_np[y:y + h, x:x + w]\n",
    "        \n",
    "        # Convert the cropped numpy array to bytes\n",
    "        cropped_img_bytes = numpy_to_bytes(cropped_img_np)\n",
    "        \n",
    "        # Extract text using EasyOCR\n",
    "        result = reader.readtext(cropped_img_bytes, detail=0)  # detail=0 to get only text\n",
    "        \n",
    "        # Draw the bounding box on the original image (for visualization)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=2)\n",
    "        \n",
    "        # Store the extracted text\n",
    "        extracted_data[f\"box_{box_id}\"] = {'text': ' '.join(result), 'bounding_box': (x, y, w, h)}\n",
    "\n",
    "    return img, extracted_data\n",
    "\n",
    "# Initialize EasyOCR reader for Gujarati\n",
    "reader = easyocr.Reader(['hi'])\n",
    "\n",
    "# Define manual bounding boxes (x, y, width, height)\n",
    "# Example boxes; replace with actual coordinates\n",
    "manual_boxes = [\n",
    "    (100, 100, 200, 50),  # Example box 1\n",
    "    (300, 200, 150, 75),  # Example box 2\n",
    "    # Add more boxes based on regions of interest\n",
    "]\n",
    "\n",
    "# Process the image and extract data from the manually defined boxes\n",
    "image_path = 'hindi2.jpeg'  # Replace with your actual image file path\n",
    "image_with_boxes, extracted_data = extract_text_with_manual_boxes(image_path, manual_boxes, reader)\n",
    "\n",
    "# Display the image with the drawn bounding boxes\n",
    "image_with_boxes.show()\n",
    "\n",
    "# Print the extracted data\n",
    "print(\"Extracted Data:\", extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581fadfe33d644118c48f16060d23e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8792768682aa4f4e8bc7d358baf0228b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42d0ba431944480bb93763071fca1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e946b3b734594b80ad37ecafd885c8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7813d75f6e6f46e2a863239a94b6932d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Telugu-LLM-Labs/Indic-gemma-7b-finetuned-sft-Navarasa-2.0\",\n",
    "    load_in_4bit = False,\n",
    "    token = \"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\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Telugu-LLM-Labs/Indic-gemma-7b-finetuned-sft-Navarasa-2.0\")\n",
    "\n",
    "input_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "input_text = input_prompt.format(\n",
    "        \"Tranlsate following sentence to Hindi.\", # instruction\n",
    "        \"India is a great country.\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "inputs = tokenizer([input_text], return_tensors = \"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>\n",
      "### Instruction:\n",
      "Tranlsate following sentence to Hindi.\n",
      "\n",
      "### Input:\n",
      "India is a great country.\n",
      "\n",
      "### Response:\n",
      "à¤­à¤¾à¤°à¤¤ à¤à¤• à¤®à¤¹à¤¾à¤¨ à¤¦à¥‡à¤¶ à¤¹à¥ˆà¥¤<eos>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = input_prompt.format(\n",
    "        \"You are Indian Geometry Expert. So give the answer of the question realted to Indian Geometry otherwise tell Sorry you don't know.\", # instruction\n",
    "        \"àª­àª°à«‚àªš àª•à«àª¯àª¾àª‚ àª›à«‡?\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>\n",
      "### Instruction:\n",
      "You are Indian Geometry Expert. So give the answer of the question realted to Indian Geometry otherwise tell Sorry you don't know.\n",
      "\n",
      "### Input:\n",
      "àª­àª°à«‚àªš àª•à«àª¯àª¾àª‚ àª›à«‡?\n",
      "\n",
      "### Response:\n",
      "àª­àª°à«‚àªš àª­àª¾àª°àª¤àª¨àª¾ àª—à«àªœàª°àª¾àª¤ àª°àª¾àªœà«àª¯àª¨àª¾ àªµàª¡à«‹àª¦àª°àª¾ àªœàª¿àª²à«àª²àª¾àª®àª¾àª‚ àª†àªµà«‡àª²à«àª‚ àªàª• àª¶àª¹à«‡àª° àª›à«‡. àª¤à«‡ àªµàª¡à«‹àª¦àª°àª¾ àªœàª¿àª²à«àª²àª¾àª¨à«àª‚ àª®à«àª–à«àª¯ àª¶àª¹à«‡àª° àª›à«‡ àª…àª¨à«‡ àª¤à«‡àª¨à«‡ àª—à«àªœàª°àª¾àª¤àª¨à«àª‚ àª¬à«€àªœà«àª‚ àª¸à«Œàª¥à«€ àª®à«‹àªŸà«àª‚ àª¶àª¹à«‡àª° àª®àª¾àª¨àªµàª¾àª®àª¾àª‚ àª†àªµà«‡ àª›à«‡. àª¤à«‡àª¨à«‡ 2011àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 2006àª®àª¾àª‚ 200\n"
     ]
    }
   ],
   "source": [
    "input_text\n",
    "inputs = tokenizer([input_text], return_tensors = \"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
